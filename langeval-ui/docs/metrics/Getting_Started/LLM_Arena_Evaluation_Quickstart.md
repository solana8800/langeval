# Báº¯t Ä‘áº§u
# TrÆ°á»ng há»£p sá»­ dá»¥ng
# LLM Arena

## HÆ°á»›ng dáº«n nhanh vá» ÄÃ¡nh giÃ¡ LLM Arena

Há»c cÃ¡ch Ä‘Ã¡nh giÃ¡ cÃ¡c phiÃªn báº£n khÃ¡c nhau cá»§a á»©ng dá»¥ng LLM cá»§a báº¡n báº±ng cÃ¡ch sá»­ dá»¥ng LLM Arena-as-a-Judge trong `deepeval`, má»™t Ä‘Ã¡nh giÃ¡ LLM dá»±a trÃªn so sÃ¡nh.

## Tá»•ng Quan

Thay vÃ¬ so sÃ¡nh cÃ¡c Ä‘áº§u ra LLM báº±ng phÆ°Æ¡ng phÃ¡p LLM-as-a-Judge má»™t Ä‘áº§u ra nhÆ° Ä‘Ã£ tháº¥y trong cÃ¡c pháº§n trÆ°á»›c, báº¡n cÅ©ng cÃ³ thá»ƒ so sÃ¡nh n test case theo cáº·p (pairwise) Ä‘á»ƒ tÃ¬m ra phiÃªn báº£n tá»‘t nháº¥t cá»§a á»©ng dá»¥ng LLM cá»§a mÃ¬nh. PhÆ°Æ¡ng phÃ¡p nÃ y máº·c dÃ¹ khÃ´ng cung cáº¥p Ä‘iá»ƒm sá»‘ báº±ng sá»‘, nhÆ°ng cho phÃ©p báº¡n chá»n Ä‘áº§u ra LLM "chiáº¿n tháº¯ng" Ä‘Ã¡ng tin cáº­y hÆ¡n cho má»™t táº­p há»£p Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra nháº¥t Ä‘á»‹nh.

**Trong hÆ°á»›ng dáº«n nhanh 5 phÃºt nÃ y, báº¡n sáº½ há»c cÃ¡ch:**

*   Thiáº¿t láº­p má»™t LLM arena
*   Sá»­ dá»¥ng Arena G-Eval Ä‘á»ƒ chá»n á»©ng dá»¥ng LLM hoáº¡t Ä‘á»™ng tá»‘t nháº¥t

## Äiá»u Kiá»‡n TiÃªn Quyáº¿t

*   CÃ i Ä‘áº·t `deepeval`
*   Má»™t khÃ³a API Confident AI (Ä‘Æ°á»£c khuyáº¿n nghá»‹). ÄÄƒng kÃ½ má»™t cÃ¡i [táº¡i Ä‘Ã¢y](https://app.confident-ai.com)

:::info
Confident AI cho phÃ©p báº¡n xem vÃ  chia sáº» cÃ¡c bÃ¡o cÃ¡o kiá»ƒm thá»­ cá»§a mÃ¬nh. Äáº·t khÃ³a API cá»§a báº¡n trong CLI:

```bash
CONFIDENT_API_KEY="confident_us..."
```
:::

## Thiáº¿t Láº­p LLM Arena

Trong `deepeval`, cÃ¡c arena test case Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ so sÃ¡nh cÃ¡c phiÃªn báº£n khÃ¡c nhau cá»§a á»©ng dá»¥ng LLM cá»§a báº¡n Ä‘á»ƒ xem phiÃªn báº£n nÃ o hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n. Má»—i test case lÃ  má»™t Ä‘áº¥u trÆ°á»ng (arena) chá»©a cÃ¡c thÃ­ sinh (contestants) khÃ¡c nhau lÃ  cÃ¡c phiÃªn báº£n khÃ¡c nhau cá»§a á»©ng dá»¥ng LLM cá»§a báº¡n, Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ dá»±a trÃªn `LLMTestCase` tÆ°Æ¡ng á»©ng cá»§a chÃºng.

:::note
`deepeval` cung cáº¥p nhiá»u lá»±a chá»n mÃ´ hÃ¬nh LLM mÃ  báº¡n cÃ³ thá»ƒ dá»… dÃ ng chá»n vÃ  cháº¡y Ä‘Ã¡nh giÃ¡ cÃ¹ng.

*   OpenAI
*   Anthropic
*   Gemini
*   Ollama
*   Grok
*   Azure OpenAI
*   Amazon Bedrock
*   Vertex AI

```python
from deepeval.metrics import ArenaGEval

task_completion_metric = ArenaGEval(model="gpt-4.1")
```

(CÃ¡c vÃ­ dá»¥ khÃ¡c tÆ°Æ¡ng tá»±...)
:::

### Táº¡o má»™t arena test case

Táº¡o má»™t `ArenaTestCase` báº±ng cÃ¡ch truyá»n vÃ o má»™t danh sÃ¡ch cÃ¡c thÃ­ sinh.

`main.py`

```python
from deepeval.test_case import ArenaTestCase, LLMTestCase, Contestant

contestant_1 = Contestant(
    name="Version 1",
    hyperparameters={"model": "gpt-3.5-turbo"},
    test_case=LLMTestCase(
        input="What is the capital of France?",
        actual_output="Paris",
    ),
)

contestant_2 = Contestant(
    name="Version 2",
    hyperparameters={"model": "gpt-4o"},
    test_case=LLMTestCase(
        input="What is the capital of France?",
        actual_output="Paris is the capital of France.",
    ),
)

contestant_3 = Contestant(
    name="Version 3",
    hyperparameters={"model": "gpt-4.1"},
    test_case=LLMTestCase(
        input="What is the capital of France?",
        actual_output="Absolutely! The capital of France is Paris ğŸ˜Š",
    ),
)

test_case = ArenaTestCase(contestants=[contestant_1, contestant_2, contestant_3])
```

Báº¡n cÃ³ thá»ƒ tÃ¬m hiá»ƒu thÃªm vá» `ArenaTestCase` [táº¡i Ä‘Ã¢y](https://deepeval.com/docs/evaluation-arena-test-cases).

### XÃ¡c Ä‘á»‹nh sá»‘ liá»‡u arena

Sá»‘ liá»‡u [`ArenaGEval`](https://deepeval.com/docs/metrics-arena-g-eval) lÃ  sá»‘ liá»‡u duy nháº¥t tÆ°Æ¡ng thÃ­ch vá»›i `ArenaTestCase`. NÃ³ chá»n má»™t ngÆ°á»i chiáº¿n tháº¯ng trong sá»‘ cÃ¡c thÃ­ sinh dá»±a trÃªn cÃ¡c tiÃªu chÃ­ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh.

```python
from deepeval.metrics import ArenaGEval
from deepeval.test_case import LLMTestCaseParams

arena_geval = ArenaGEval(
    name="Friendly",
    criteria="Choose the winner of the more friendly contestant based on the input and actual output",
    evaluation_params=[
        LLMTestCaseParams.INPUT,
        LLMTestCaseParams.ACTUAL_OUTPUT,
    ]
)
```

## Cháº¡y ÄÃ¡nh GiÃ¡ Arena Äáº§u TiÃªn Cá»§a Báº¡n

BÃ¢y giá» báº¡n Ä‘Ã£ táº¡o má»™t arena vá»›i cÃ¡c thÃ­ sinh vÃ  xÃ¡c Ä‘á»‹nh má»™t sá»‘ liá»‡u, báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u cháº¡y cÃ¡c Ä‘Ã¡nh giÃ¡ arena Ä‘á»ƒ xÃ¡c Ä‘á»‹nh thÃ­ sinh chiáº¿n tháº¯ng.

### Cháº¡y má»™t Ä‘Ã¡nh giÃ¡

Báº¡n cÃ³ thá»ƒ cháº¡y cÃ¡c Ä‘Ã¡nh giÃ¡ arena báº±ng cÃ¡ch sá»­ dá»¥ng hÃ m `compare()`.

`main.py`

```python
from deepeval.test_case import ArenaTestCase, LLMTestCase, LLMTestCaseParams
from deepeval.metrics import ArenaGEval
from deepeval import compare

test_case = ArenaTestCase(
    contestants=[...], # Sá»­ dá»¥ng cÃ¡c thÃ­ sinh tÆ°Æ¡ng tá»± báº¡n Ä‘Ã£ táº¡o trÆ°á»›c Ä‘Ã³
)

arena_geval = ArenaGEval(...) # Sá»­ dá»¥ng sá»‘ liá»‡u tÆ°Æ¡ng tá»± báº¡n Ä‘Ã£ táº¡o trÆ°á»›c Ä‘Ã³

compare(test_cases=[test_case], metric=arena_geval)
```

**Ghi log prompt vÃ  model**

Báº¡n cÃ³ thá»ƒ tÃ¹y chá»n ghi log cÃ¡c prompt vÃ  model cho má»—i thÃ­ sinh thÃ´ng qua tá»« Ä‘iá»ƒn `hyperparameters` trong hÃ m `compare()`. Äiá»u nÃ y sáº½ cho phÃ©p báº¡n dá»… dÃ ng quy káº¿t cÃ¡c thÃ­ sinh chiáº¿n tháº¯ng cho cÃ¡c siÃªu tham sá»‘ tÆ°Æ¡ng á»©ng cá»§a há».

```python
from deepeval.prompt import Prompt

prompt_1 = Prompt(
    alias="First Prompt",
    messages_template=[PromptMessage(role="system", content="You are a helpful assistant.")]
)
prompt_2 = Prompt(
    alias="Second Prompt",
    messages_template=[PromptMessage(role="system", content="You are a helpful assistant.")]
)

compare(
    test_cases=[test_case],
    metric=arena_geval,
    hyperparameters={
        "Version 1": {"prompt": prompt_1},
        "Version 2": {"prompt": prompt_2},
    },
)
```

BÃ¢y giá» báº¡n cÃ³ thá»ƒ cháº¡y file python nÃ y Ä‘á»ƒ nháº­n káº¿t quáº£:

```bash
python main.py
```

Äiá»u nÃ y sáº½ cho phÃ©p báº¡n xem káº¿t quáº£ cá»§a arena nhÆ° hiá»ƒn thá»‹ bÃªn dÆ°á»›i:

```python
Counter({'Version 3': 1})
```

ğŸ‰ğŸ¥³ **ChÃºc má»«ng!** Báº¡n vá»«a cháº¡y Ä‘Ã¡nh giÃ¡ dá»±a trÃªn LLM arena Ä‘áº§u tiÃªn cá»§a mÃ¬nh. ÄÃ¢y lÃ  nhá»¯ng gÃ¬ Ä‘Ã£ xáº£y ra:

*   Khi báº¡n gá»i `compare()`, `deepeval` láº·p qua tá»«ng `ArenaTestCase`
*   Äá»‘i vá»›i má»—i test case, `deepeval` sá»­ dá»¥ng sá»‘ liá»‡u `ArenaGEval` Ä‘á»ƒ chá»n "ngÆ°á»i chiáº¿n tháº¯ng"
*   Äá»ƒ lÃ m cho arena khÃ´ng thiÃªn vá»‹, `deepeval` che tÃªn cá»§a tá»«ng thÃ­ sinh vÃ  ngáº«u nhiÃªn hÃ³a vá»‹ trÃ­ cá»§a há»
*   Cuá»‘i cÃ¹ng, báº¡n nháº­n Ä‘Æ°á»£c sá»‘ lÆ°á»£ng "chiáº¿n tháº¯ng" mÃ  má»—i thÃ­ sinh cÃ³ Ä‘Æ°á»£c dÆ°á»›i dáº¡ng Ä‘áº§u ra cuá»‘i cÃ¹ng.

KhÃ´ng giá»‘ng nhÆ° LLM-as-a-Judge má»™t Ä‘áº§u ra (lÃ  má»i thá»© trá»« Ä‘Ã¡nh giÃ¡ LLM arena), khÃ¡i niá»‡m vá» má»™t test case "vÆ°á»£t qua" (passing) khÃ´ng tá»“n táº¡i Ä‘á»‘i vá»›i cÃ¡c Ä‘Ã¡nh giÃ¡ arena.

### Xem trÃªn Confident AI (Ä‘Æ°á»£c khuyáº¿n nghá»‹)

Náº¿u báº¡n Ä‘Ã£ Ä‘áº·t `CONFIDENT_API_KEY`, cÃ¡c so sÃ¡nh arena cá»§a báº¡n sáº½ tá»± Ä‘á»™ng xuáº¥t hiá»‡n dÆ°á»›i dáº¡ng má»™t thá»­ nghiá»‡m (experiment) trÃªn [Confident AI](https://app.confident-ai.com), ná»n táº£ng DeepEval.

[](https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started%3Aarena-evals%3Aexperiment.mp4)

## CÃ¡c BÆ°á»›c Tiáº¿p Theo

`deepeval` cho phÃ©p báº¡n cháº¡y cÃ¡c so sÃ¡nh Arena cá»¥c bá»™ nhÆ°ng khÃ´ng Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho cÃ¡c cáº£i tiáº¿n prompt hoáº·c mÃ´ hÃ¬nh láº·p Ä‘i láº·p láº¡i. Náº¿u báº¡n Ä‘ang tÃ¬m kiáº¿m má»™t cÃ¡ch toÃ n diá»‡n vÃ  há»£p lÃ½ hÆ¡n Ä‘á»ƒ cháº¡y cÃ¡c so sÃ¡nh Arena, [**Confident AI**](https://app.confident-ai.com) cho phÃ©p báº¡n dá»… dÃ ng kiá»ƒm thá»­ cÃ¡c prompt, model, cÃ´ng cá»¥ vÃ  cáº¥u hÃ¬nh Ä‘áº§u ra khÃ¡c nhau **song song**, vÃ  Ä‘Ã¡nh giÃ¡ chÃºng báº±ng cÃ¡ch sá»­ dá»¥ng báº¥t ká»³ sá»‘ liá»‡u `deepeval` nÃ o ngoÃ i `ArenaGEval`â€”táº¥t cáº£ trá»±c tiáº¿p trÃªn ná»n táº£ng.

*   So sÃ¡nh Nhanh
*   Thá»­ Nghiá»‡m (Experiments)
*   So sÃ¡nh cÃ³ Trace
*   So sÃ¡nh Sá»‘ Liá»‡u
*   Ghi Log Prompt vÃ  Model

So sÃ¡nh cÃ¡c Ä‘áº§u ra mÃ´ hÃ¬nh trá»±c tiáº¿p báº±ng cÃ¡ch sá»­ dá»¥ng Ä‘Ã¡nh giÃ¡ arena.

[](https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started%3Aarena-evals%3Aquick-run.mp4)

Táº¡o má»™t thá»­ nghiá»‡m Ä‘á»ƒ cháº¡y cÃ¡c so sÃ¡nh toÃ n diá»‡n trÃªn má»™t bá»™ dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ vÃ  táº­p há»£p cÃ¡c sá»‘ liá»‡u.

[](https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started%3Aarena-evals%3Arun-experiment.mp4)

Xem chi tiáº¿t dáº¥u váº¿t cá»§a cÃ¡c lá»‡nh gá»i LLM vÃ  cÃ´ng cá»¥ trong quÃ¡ trÃ¬nh so sÃ¡nh mÃ´ hÃ¬nh.

[](https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started%3Aarena-evals%3Atraced-comparisons.mp4)

Ãp dá»¥ng cÃ¡c sá»‘ liá»‡u Ä‘Ã¡nh giÃ¡ tÃ¹y chá»‰nh Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c mÃ´ hÃ¬nh chiáº¿n tháº¯ng trong cÃ¡c so sÃ¡nh Ä‘á»‘i Ä‘áº§u.

[](https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started%3Aarena-evals%3Ametric-comparisons.mp4)

Theo dÃµi cÃ¡c prompt vÃ  cáº¥u hÃ¬nh mÃ´ hÃ¬nh Ä‘á»ƒ hiá»ƒu siÃªu tham sá»‘ nÃ o dáº«n Ä‘áº¿n hiá»‡u suáº¥t tá»‘t hÆ¡n.

[](https://deepeval-docs.s3.us-east-1.amazonaws.com/getting-started%3Aarena-evals%3Alog-prompts.mp4)

BÃ¢y giá» báº¡n Ä‘Ã£ cháº¡y cÃ¡c Ä‘Ã¡nh giÃ¡ Arena Ä‘áº§u tiÃªn cá»§a mÃ¬nh, báº¡n nÃªn:

1.  **TÃ¹y chá»‰nh cÃ¡c sá»‘ liá»‡u cá»§a báº¡n**: Báº¡n cÃ³ thá»ƒ thay Ä‘á»•i cÃ¡c tiÃªu chÃ­ cá»§a sá»‘ liá»‡u Ä‘á»ƒ cá»¥ thá»ƒ hÆ¡n cho trÆ°á»ng há»£p sá»­ dá»¥ng cá»§a báº¡n.
2.  **Chuáº©n bá»‹ má»™t bá»™ dá»¯ liá»‡u**: Náº¿u báº¡n khÃ´ng cÃ³, hÃ£y [táº¡o má»™t bá»™](/docs/synthesizer-introduction) lÃ m Ä‘iá»ƒm khá»Ÿi Ä‘áº§u Ä‘á»ƒ lÆ°u trá»¯ Ä‘áº§u vÃ o cá»§a báº¡n dÆ°á»›i dáº¡ng goldens.

Sá»‘ liá»‡u arena chá»‰ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ chá»n ngÆ°á»i chiáº¿n tháº¯ng trong sá»‘ cÃ¡c thÃ­ sinh, nÃ³ khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ chÃ­nh cÃ¡c cÃ¢u tráº£ lá»i. Äá»ƒ Ä‘Ã¡nh giÃ¡ á»©ng dá»¥ng LLM cá»§a báº¡n trÃªn cÃ¡c trÆ°á»ng há»£p sá»­ dá»¥ng cá»¥ thá»ƒ, báº¡n cÃ³ thá»ƒ Ä‘á»c cÃ¡c hÆ°á»›ng dáº«n nhanh khÃ¡c táº¡i Ä‘Ã¢y:

[AI Agents](/docs/getting-started-agents)
*   Thiáº¿t láº­p LLM tracing
*   Kiá»ƒm thá»­ hoÃ n thÃ nh tÃ¡c vá»¥ end-to-end
*   ÄÃ¡nh giÃ¡ cÃ¡c thÃ nh pháº§n riÃªng láº»

[RAG](/docs/getting-started-rag)
*   ÄÃ¡nh giÃ¡ RAG end-to-end
*   Kiá»ƒm thá»­ retriever vÃ  generator riÃªng biá»‡t
*   ÄÃ¡nh giÃ¡ RAG nhiá»u lÆ°á»£t

[Chatbots](/docs/getting-started-chatbots)
*   Thiáº¿t láº­p test case nhiá»u lÆ°á»£t
*   ÄÃ¡nh giÃ¡ cÃ¡c lÆ°á»£t trong má»™t cuá»™c há»™i thoáº¡i
*   MÃ´ phá»ng tÆ°Æ¡ng tÃ¡c ngÆ°á»i dÃ¹ng

[Chá»‰nh sá»­a trang nÃ y](https://github.com/confident-ai/deepeval/edit/main/docs/docs/getting-started-llm-arena.mdx)

Cáº­p nháº­t láº§n cuá»‘i vÃ o **9 thÃ¡ng 1, 2026** bá»Ÿi **Jeffrey Ip**
