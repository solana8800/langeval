# Báº¯t Ä‘áº§u
# Giá»›i thiá»‡u nhanh

## Giá»›i thiá»‡u nhanh

**DeepEval** lÃ  má»™t framework Ä‘Ã¡nh giÃ¡ mÃ£ nguá»“n má»Ÿ dÃ nh cho cÃ¡c LLM. DeepEval giÃºp viá»‡c xÃ¢y dá»±ng vÃ  láº·p láº¡i cÃ¡c á»©ng dá»¥ng LLM trá»Ÿ nÃªn cá»±c ká»³ dá»… dÃ ng vÃ  Ä‘Æ°á»£c xÃ¢y dá»±ng vá»›i cÃ¡c nguyÃªn táº¯c sau:

*   Dá»… dÃ ng "kiá»ƒm thá»­ Ä‘Æ¡n vá»‹" (unit test) cÃ¡c Ä‘áº§u ra cá»§a LLM theo cÃ¡ch tÆ°Æ¡ng tá»± nhÆ° Pytest.
*   Sá»­ dá»¥ng ngay hÆ¡n 50 sá»‘ liá»‡u Ä‘Ã¡nh giÃ¡ LLM (metrics), háº§u háº¿t Ä‘á»u cÃ³ cÆ¡ sá»Ÿ nghiÃªn cá»©u vÃ  táº¥t cáº£ Ä‘á»u Ä‘a phÆ°Æ¡ng thá»©c (multi-modal).
*   ÄÃ¡nh giÃ¡ cho RAG, agents, chatbots vÃ  háº§u nhÆ° má»i trÆ°á»ng há»£p sá»­ dá»¥ng.
*   Há»— trá»£ Ä‘Ã¡nh giÃ¡ cáº£ end-to-end (tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i) vÃ  cáº¥p Ä‘á»™ thÃ nh pháº§n (component level).
*   Táº¡o bá»™ dá»¯ liá»‡u tá»•ng há»£p vá»›i cÃ¡c ká»¹ thuáº­t tiáº¿n hÃ³a tiÃªn tiáº¿n nháº¥t.
*   CÃ¡c sá»‘ liá»‡u (metrics) Ä‘Æ¡n giáº£n Ä‘á»ƒ tÃ¹y chá»‰nh vÃ  bao gá»“m táº¥t cáº£ cÃ¡c trÆ°á»ng há»£p sá»­ dá»¥ng.
*   Red team, quÃ©t an toÃ n cÃ¡c á»©ng dá»¥ng LLM Ä‘á»ƒ tÃ¬m cÃ¡c lá»— há»•ng báº£o máº­t.

NgoÃ i ra, DeepEval cÃ³ má»™t ná»n táº£ng Ä‘Ã¡m mÃ¢y [Confident AI](https://app.confident-ai.com), cho phÃ©p cÃ¡c nhÃ³m sá»­ dá»¥ng DeepEval Ä‘á»ƒ **Ä‘Ã¡nh giÃ¡, kiá»ƒm thá»­ há»“i quy (regression test), red team vÃ  giÃ¡m sÃ¡t** cÃ¡c á»©ng dá»¥ng LLM trÃªn Ä‘Ã¡m mÃ¢y.

ÄÆ°á»£c cung cáº¥p bá»Ÿi

![](../images/9b2c4d8d.svg)

Confident AI

## CÃ i Ä‘áº·t

Trong má»™t mÃ´i trÆ°á»ng áº£o má»›i Ä‘Æ°á»£c táº¡o, hÃ£y cháº¡y:

```bash
pip install -U deepeval
```

`deepeval` cháº¡y cÃ¡c Ä‘Ã¡nh giÃ¡ cá»¥c bá»™ trÃªn mÃ´i trÆ°á»ng cá»§a báº¡n. Äá»ƒ giá»¯ cÃ¡c bÃ¡o cÃ¡o kiá»ƒm thá»­ cá»§a báº¡n á»Ÿ má»™t nÆ¡i táº­p trung trÃªn Ä‘Ã¡m mÃ¢y, hÃ£y sá»­ dá»¥ng [Confident AI](https://www.confident-ai.com), ná»n táº£ng Ä‘Ã¡nh giÃ¡ gá»‘c cho DeepEval:

```bash
deepeval login
```

### Cáº¥u hÃ¬nh Biáº¿n MÃ´i trÆ°á»ng

DeepEval tá»± Ä‘á»™ng táº£i cÃ¡c file mÃ´i trÆ°á»ng (táº¡i thá»i Ä‘iá»ƒm import)

*   **Äá»™ Æ°u tiÃªn:** env cá»§a tiáº¿n trÃ¬nh hiá»‡n táº¡i -> `.env.local` -> `.env`
*   **Há»§y kÃ­ch hoáº¡t:** Ä‘áº·t `DEEPEVAL_DISABLE_DOTENV=1`

ThÃ´ng tin thÃªm vá» cÃ¡c cÃ i Ä‘áº·t `env` cÃ³ thá»ƒ Ä‘Æ°á»£c [tÃ¬m tháº¥y táº¡i Ä‘Ã¢y.](/docs/evaluation-flags-and-configs#environment-flags)

```bash
# báº¯t Ä‘áº§u nhanh
cp .env.example .env.local
# sau Ä‘Ã³ chá»‰nh sá»­a .env.local (Ä‘Æ°á»£c git bá» qua)
```

:::note
Confident AI miá»…n phÃ­ vÃ  cho phÃ©p báº¡n giá»¯ táº¥t cáº£ káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn Ä‘Ã¡m mÃ¢y. ÄÄƒng kÃ½ [táº¡i Ä‘Ã¢y.](https://app.confident-ai.com)
:::

## Táº¡o Láº§n Cháº¡y Kiá»ƒm Thá»­ Äáº§u TiÃªn Cá»§a Báº¡n

Táº¡o má»™t file kiá»ƒm thá»­ Ä‘á»ƒ cháº¡y **Ä‘Ã¡nh giÃ¡ end-to-end** Ä‘áº§u tiÃªn cá»§a báº¡n.

*   Single-Turn (Má»™t lÆ°á»£t)
*   Multi-Turn (Nhiá»u lÆ°á»£t)

Má»™t [LLM test case](/docs/evaluation-test-cases#llm-test-case) trong `deepeval` Ä‘áº¡i diá»‡n cho má»™t **Ä‘Æ¡n vá»‹ tÆ°Æ¡ng tÃ¡c á»©ng dá»¥ng LLM duy nháº¥t**, vÃ  chá»©a cÃ¡c trÆ°á»ng báº¯t buá»™c nhÆ° `input` vÃ  `actual_output` (Ä‘áº§u ra do LLM táº¡o), vÃ  cÃ¡c trÆ°á»ng tÃ¹y chá»n nhÆ° `expected_output`.

![LLM Test Case](../images/d28073bd.png)

Cháº¡y `touch test_example.py` trong terminal cá»§a báº¡n vÃ  dÃ¡n Ä‘oáº¡n mÃ£ sau vÃ o:

`test_example.py`

```python
from deepeval import assert_test
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.metrics import GEval

def test_correctness():
    correctness_metric = GEval(
        name="Correctness",
        criteria="Determine if the 'actual output' is correct based on the 'expected output'.",
        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
        threshold=0.5
    )
    test_case = LLMTestCase(
        input="I have a persistent cough and fever. Should I be worried?",
        # Thay tháº¿ pháº§n nÃ y báº±ng Ä‘áº§u ra thá»±c táº¿ tá»« á»©ng dá»¥ng LLM cá»§a báº¡n
        actual_output="A persistent cough and fever could be a viral infection or something more serious. See a doctor if symptoms worsen or don't improve in a few days.",
        expected_output="A persistent cough and fever could indicate a range of illnesses, from a mild viral infection to more serious conditions like pneumonia or COVID-19. You should seek medical attention if your symptoms worsen, persist for more than a few days, or are accompanied by difficulty breathing, chest pain, or other concerning signs."
    )
    assert_test(test_case, [correctness_metric])
```

Sau Ä‘Ã³, cháº¡y `deepeval test run` tá»« thÆ° má»¥c gá»‘c cá»§a dá»± Ã¡n Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ á»©ng dá»¥ng LLM cá»§a báº¡n **end-to-end**:

```bash
deepeval test run test_example.py
```

ChÃºc má»«ng! Test case cá»§a báº¡n Ä‘Ã£ vÆ°á»£t qua âœ… HÃ£y cÃ¹ng phÃ¢n tÃ­ch nhá»¯ng gÃ¬ Ä‘Ã£ xáº£y ra.

*   Biáº¿n `input` mÃ´ phá»ng Ä‘áº§u vÃ o cá»§a ngÆ°á»i dÃ¹ng, vÃ  `actual_output` lÃ  trÃ¬nh giá»¯ chá»— cho nhá»¯ng gÃ¬ á»©ng dá»¥ng cá»§a báº¡n Ä‘Æ°á»£c cho lÃ  sáº½ xuáº¥t ra dá»±a trÃªn Ä‘áº§u vÃ o nÃ y.
*   Biáº¿n `expected_output` Ä‘áº¡i diá»‡n cho cÃ¢u tráº£ lá»i lÃ½ tÆ°á»Ÿng cho má»™t `input` nháº¥t Ä‘á»‹nh, vÃ  [`GEval`](/docs/metrics-llm-evals) lÃ  má»™t sá»‘ liá»‡u Ä‘Æ°á»£c há»— trá»£ bá»Ÿi nghiÃªn cá»©u do `deepeval` cung cáº¥p Ä‘á»ƒ báº¡n Ä‘Ã¡nh giÃ¡ Ä‘áº§u ra cá»§a LLM trÃªn báº¥t ká»³ sá»‘ liá»‡u tÃ¹y chá»‰nh nÃ o vá»›i Ä‘á»™ chÃ­nh xÃ¡c giá»‘ng nhÆ° con ngÆ°á»i.
*   Trong vÃ­ dá»¥ nÃ y, tiÃªu chÃ­ (`criteria`) cá»§a sá»‘ liá»‡u lÃ  tÃ­nh Ä‘Ãºng Ä‘áº¯n cá»§a `actual_output` dá»±a trÃªn `expected_output` Ä‘Æ°á»£c cung cáº¥p, nhÆ°ng khÃ´ng pháº£i táº¥t cáº£ cÃ¡c sá»‘ liá»‡u Ä‘á»u yÃªu cáº§u `expected_output`.
*   Táº¥t cáº£ cÃ¡c Ä‘iá»ƒm sá»‘ liá»‡u náº±m trong khoáº£ng tá»« 0 - 1, trong Ä‘Ã³ ngÆ°á»¡ng `threshold=0.5` cuá»‘i cÃ¹ng xÃ¡c Ä‘á»‹nh xem bÃ i kiá»ƒm tra cá»§a báº¡n cÃ³ vÆ°á»£t qua hay khÃ´ng.

Náº¿u báº¡n cháº¡y nhiá»u hÆ¡n má»™t láº§n cháº¡y kiá»ƒm thá»­, báº¡n sáº½ cÃ³ thá»ƒ **báº¯t Ä‘Æ°á»£c cÃ¡c há»“i quy (regressions)** báº±ng cÃ¡ch so sÃ¡nh cÃ¡c test case song song. Äiá»u nÃ y cÅ©ng dá»… dÃ ng hÆ¡n náº¿u báº¡n Ä‘ang sá»­ dá»¥ng `deepeval` cÃ¹ng vá»›i Confident AI ([xem bÃªn dÆ°á»›i](/docs/getting-started#save-results-on-cloud) Ä‘á»ƒ xem video demo).

Má»™t [conversational test case](/docs/evaluation-multiturn-test-cases#conversational-test-case) trong `deepeval` Ä‘áº¡i diá»‡n cho má»™t **tÆ°Æ¡ng tÃ¡c nhiá»u lÆ°á»£t vá»›i á»©ng dá»¥ng LLM cá»§a báº¡n**, vÃ  chá»©a thÃ´ng tin nhÆ° cuá»™c há»™i thoáº¡i thá»±c táº¿ Ä‘Ã£ diá»…n ra dÆ°á»›i dáº¡ng cÃ¡c lÆ°á»£t (`turn`s), vÃ  tÃ¹y chá»n ká»‹ch báº£n mÃ  cuá»™c há»™i thoáº¡i Ä‘Ã£ xáº£y ra.

![Conversational Test Case](../images/7ceb025e.png)

Cháº¡y `touch test_example.py` trong terminal cá»§a báº¡n vÃ  dÃ¡n Ä‘oáº¡n mÃ£ sau vÃ o:

`test_example.py`

```python
from deepeval import assert_test
from deepeval.test_case import Turn, ConversationalTestCase
from deepeval.metrics import ConversationalGEval

def test_professionalism():
    professionalism_metric = ConversationalGEval(
        name="Professionalism",
        criteria="Determine whether the assistant has acted professionally based on the content.",
        threshold=0.5
    )
    test_case = ConversationalTestCase(
        turns=[
            Turn(role="user", content="What is DeepEval?"),
            Turn(role="assistant", content="DeepEval is an open-source LLM eval package.")
        ]
    )
    assert_test(test_case, [professionalism_metric])
```

Sau Ä‘Ã³, cháº¡y `deepeval test run` tá»« thÆ° má»¥c gá»‘c cá»§a dá»± Ã¡n Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ á»©ng dá»¥ng LLM cá»§a báº¡n **end-to-end**:

```bash
deepeval test run test_example.py
```

ğŸ‰ ChÃºc má»«ng! Test case cá»§a báº¡n Ä‘Ã£ vÆ°á»£t qua âœ… HÃ£y cÃ¹ng phÃ¢n tÃ­ch nhá»¯ng gÃ¬ Ä‘Ã£ xáº£y ra.

*   Biáº¿n `role` phÃ¢n biá»‡t giá»¯a ngÆ°á»i dÃ¹ng cuá»‘i vÃ  á»©ng dá»¥ng LLM cá»§a báº¡n, vÃ  `content` chá»©a Ä‘áº§u vÃ o cá»§a ngÆ°á»i dÃ¹ng hoáº·c Ä‘áº§u ra cá»§a LLM.
*   Trong vÃ­ dá»¥ nÃ y, tiÃªu chÃ­ (`criteria`) sá»‘ liá»‡u Ä‘Ã¡nh giÃ¡ tÃ­nh chuyÃªn nghiá»‡p cá»§a chuá»—i `content`.
*   Táº¥t cáº£ cÃ¡c Ä‘iá»ƒm sá»‘ liá»‡u náº±m trong khoáº£ng tá»« 0 - 1, trong Ä‘Ã³ ngÆ°á»¡ng `threshold=0.5` cuá»‘i cÃ¹ng xÃ¡c Ä‘á»‹nh xem bÃ i kiá»ƒm tra cá»§a báº¡n cÃ³ vÆ°á»£t qua hay khÃ´ng.

Náº¿u báº¡n cháº¡y nhiá»u hÆ¡n má»™t láº§n cháº¡y kiá»ƒm thá»­, báº¡n sáº½ cÃ³ thá»ƒ **báº¯t Ä‘Æ°á»£c cÃ¡c há»“i quy (regressions)** báº±ng cÃ¡ch so sÃ¡nh cÃ¡c test case song song. Äiá»u nÃ y cÅ©ng dá»… dÃ ng hÆ¡n náº¿u báº¡n Ä‘ang sá»­ dá»¥ng `deepeval` cÃ¹ng vá»›i Confident AI ([xem bÃªn dÆ°á»›i](/docs/getting-started#save-results-on-cloud) Ä‘á»ƒ xem video demo).

:::info
VÃ¬ háº§u háº¿t cÃ¡c sá»‘ liá»‡u cá»§a `deepeval` bao gá»“m cáº£ `GEval` Ä‘á»u lÃ  cÃ¡c sá»‘ liá»‡u LLM-as-a-Judge, báº¡n sáº½ cáº§n Ä‘áº·t `OPENAI_API_KEY` cá»§a mÃ¬nh lÃ m biáº¿n mÃ´i trÆ°á»ng. Báº¡n cÅ©ng cÃ³ thá»ƒ tÃ¹y chá»‰nh mÃ´ hÃ¬nh Ä‘Æ°á»£c sá»­ dá»¥ng cho cÃ¡c Ä‘Ã¡nh giÃ¡:

```python
correctness_metric = GEval(..., model="o1")
```

DeepEval cÅ©ng tÃ­ch há»£p vá»›i cÃ¡c nhÃ  cung cáº¥p mÃ´ hÃ¬nh sau: [Ollama](https://deepeval.com/integrations/models/ollama), [Azure OpenAI](https://deepeval.com/integrations/models/azure-openai), [Anthropic](https://deepeval.com/integrations/models/anthropic), [Gemini](https://deepeval.com/integrations/models/gemini), v.v. Äá»ƒ sá»­ dá»¥ng **Báº¤T Ká»²** LLM tÃ¹y chá»‰nh nÃ o báº¡n chá»n, [hÃ£y xem pháº§n nÃ y cá»§a tÃ i liá»‡u](/guides/guides-using-custom-llms).
:::

### ÄÃ¡nh giÃ¡ bá»‹ "káº¹t"?

Ráº¥t cÃ³ thá»ƒ LLM Ä‘Ã¡nh giÃ¡ cá»§a báº¡n Ä‘ang tháº¥t báº¡i vÃ  Ä‘iá»u nÃ y cÃ³ thá»ƒ do giá»›i háº¡n tá»‘c Ä‘á»™ (rate limits) hoáº·c khÃ´ng Ä‘á»§ háº¡n ngáº¡ch. Theo máº·c Ä‘á»‹nh, `deepeval` thá»­ láº¡i cÃ¡c lá»—i LLM **thoÃ¡ng qua** má»™t láº§n (tá»•ng cá»™ng 2 láº§n thá»­):

*   **ÄÃ£ thá»­ láº¡i:** lá»—i máº¡ng/timeout vÃ  lá»—i mÃ¡y chá»§ **5xx**.
*   **Giá»›i háº¡n tá»‘c Ä‘á»™ (429):** Ä‘Æ°á»£c thá»­ láº¡i trá»« khi nhÃ  cung cáº¥p Ä‘Ã¡nh dáº¥u chÃºng lÃ  khÃ´ng thá»ƒ thá»­ láº¡i (Ä‘á»‘i vá»›i OpenAI, `insufficient_quota` Ä‘Æ°á»£c coi lÃ  khÃ´ng thá»ƒ thá»­ láº¡i).
*   **Backoff:** hÃ m mÅ© vá»›i jitter (ban Ä‘áº§u **1s**, cÆ¡ sá»Ÿ **2**, jitter **2s**, tá»‘i Ä‘a **5s**).

Báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh cÃ¡c Ä‘iá»u nÃ y thÃ´ng qua cá» mÃ´i trÆ°á»ng (khÃ´ng thay Ä‘á»•i mÃ£). Xem [biáº¿n mÃ´i trÆ°á»ng](/docs/environment-variables) Ä‘á»ƒ biáº¿t chi tiáº¿t.

### LÆ°u Káº¿t Quáº£

Báº¡n nÃªn quáº£n lÃ½ bá»™ kiá»ƒm thá»­ cá»§a mÃ¬nh trÃªn Confident AI, ná»n táº£ng cá»§a `deepeval`.

*   Confident AI
*   Cá»¥c bá»™ trong JSON

Confident AI lÃ  Ä‘Ã¡m mÃ¢y `deepeval`, vÃ  giÃºp báº¡n xÃ¢y dá»±ng pipeline Ä‘Ã¡nh giÃ¡ LLM tá»‘t nháº¥t. Cháº¡y `deepeval view` Ä‘á»ƒ xem láº§n cháº¡y kiá»ƒm thá»­ má»›i cháº¡y cá»§a báº¡n trÃªn ná»n táº£ng:

```bash
deepeval view
```

Lá»‡nh `deepeval view` yÃªu cáº§u láº§n cháº¡y kiá»ƒm thá»­ mÃ  báº¡n Ä‘Ã£ cháº¡y á»Ÿ trÃªn pháº£i Ä‘Æ°á»£c lÆ°u trá»¯ thÃ nh cÃ´ng cá»¥c bá»™. Náº¿u cÃ³ lá»—i xáº£y ra, chá»‰ cáº§n cháº¡y má»™t láº§n cháº¡y kiá»ƒm thá»­ má»›i sau khi Ä‘Äƒng nháº­p báº±ng `deepeval login`:

```bash
deepeval login
```

Sau khi báº¡n Ä‘Ã£ dÃ¡n khÃ³a API cá»§a mÃ¬nh, Confident AI sáº½ **táº¡o bÃ¡o cÃ¡o kiá»ƒm thá»­ vÃ  tá»± Ä‘á»™ng hÃ³a kiá»ƒm thá»­ há»“i quy** báº¥t cá»© khi nÃ o báº¡n cháº¡y má»™t láº§n cháº¡y kiá»ƒm thá»­ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ á»©ng dá»¥ng LLM cá»§a báº¡n trong báº¥t ká»³ mÃ´i trÆ°á»ng nÃ o, á»Ÿ báº¥t ká»³ quy mÃ´ nÃ o, á»Ÿ báº¥t ká»³ Ä‘Ã¢u.

[](https://confident-docs.s3.us-east-1.amazonaws.com/evaluation:overview.mp4)

[Xem HÆ°á»›ng Dáº«n Äáº§y Äá»§ trÃªn Confident AI](https://confident-docs.s3.us-east-1.amazonaws.com/evaluation:overview.mp4)

**Khi báº¡n Ä‘Ã£ cháº¡y nhiá»u hÆ¡n má»™t láº§n cháº¡y kiá»ƒm thá»­**, báº¡n sáº½ cÃ³ thá»ƒ sá»­ dá»¥ng [trang kiá»ƒm thá»­ há»“i quy](https://www.confident-ai.com/docs/llm-evaluation/dashboards/ab-regression-testing) Ä‘Æ°á»£c hiá»ƒn thá»‹ á»Ÿ gáº§n cuá»‘i video. CÃ¡c hÃ ng mÃ u xanh lÃ¡ cÃ¢y cho biáº¿t LLM cá»§a báº¡n Ä‘Ã£ cho tháº¥y sá»± cáº£i thiá»‡n trÃªn cÃ¡c test case cá»¥ thá»ƒ, trong khi cÃ¡c hÃ ng mÃ u Ä‘á» lÃ m ná»•i báº­t cÃ¡c khu vá»±c bá»‹ há»“i quy.

Chá»‰ cáº§n Ä‘áº·t biáº¿n mÃ´i trÆ°á»ng `DEEPEVAL_RESULTS_FOLDER` thÃ nh Ä‘Æ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i báº¡n chá»n.

```bash
# linux
export DEEPEVAL_RESULTS_FOLDER="./data"

# or windows
set DEEPEVAL_RESULTS_FOLDER=.\data
```

## Cháº¡y Kiá»ƒm Thá»­ Vá»›i LLM Tracing

Trong khi cÃ¡c Ä‘Ã¡nh giÃ¡ end-to-end coi á»©ng dá»¥ng LLM cá»§a báº¡n nhÆ° má»™t há»™p Ä‘en, báº¡n cÅ©ng Ä‘Ã¡nh giÃ¡ **cÃ¡c thÃ nh pháº§n riÃªng láº»** trong á»©ng dá»¥ng LLM cá»§a mÃ¬nh thÃ´ng qua **LLM tracing**. ÄÃ¢y lÃ  cÃ¡ch Ä‘Æ°á»£c khuyáº¿n nghá»‹ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c AI agent.

![component level evals](../images/8f94fbce.png)

Äáº§u tiÃªn hÃ£y dÃ¡n Ä‘oáº¡n mÃ£ sau:

`main.py`

```python
from deepeval.tracing import observe, update_current_span
from deepeval.test_case import LLMTestCase
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.metrics import AnswerRelevancyMetric

# 1. Decorate á»©ng dá»¥ng cá»§a báº¡n
@observe()
def llm_app(input: str):
  # 2. Decorate cÃ¡c thÃ nh pháº§n vá»›i cÃ¡c sá»‘ liá»‡u báº¡n muá»‘n Ä‘Ã¡nh giÃ¡ hoáº·c gá»¡ lá»—i
  @observe(metrics=[AnswerRelevancyMetric()])
  def inner_component():
      # 3. Táº¡o test case táº¡i thá»i gian cháº¡y
      update_current_span(test_case=LLMTestCase(input="Why is the blue sky?", actual_output="You mean why is the sky blue?"))

  return inner_component()

# 4. Táº¡o bá»™ dá»¯ liá»‡u
dataset = EvaluationDataset(goldens=[Golden(input="Test input")])

# 5. Láº·p qua bá»™ dá»¯ liá»‡u
for golden in dataset.evals_iterator():
  # 6. Gá»i á»©ng dá»¥ng LLM
  llm_app(golden.input)
```

Sau Ä‘Ã³ cháº¡y `python main.py` Ä‘á»ƒ cháº¡y má»™t Ä‘Ã¡nh giÃ¡ **cáº¥p Ä‘á»™ thÃ nh pháº§n (component-level)**:

```bash
python main.py
```

ğŸ‰ ChÃºc má»«ng! Test case cá»§a báº¡n sáº½ láº¡i vÆ°á»£t qua âœ… HÃ£y cÃ¹ng phÃ¢n tÃ­ch nhá»¯ng gÃ¬ Ä‘Ã£ xáº£y ra.

*   Decorator `@observe` cho `deepeval` biáº¿t má»—i thÃ nh pháº§n á»Ÿ Ä‘Ã¢u vÃ  **táº¡o má»™t LLM trace** táº¡i thá»i gian thá»±c thi.
*   Báº¥t ká»³ `metrics` nÃ o Ä‘Æ°á»£c cung cáº¥p cho `@observe` Ä‘á»u cho phÃ©p `deepeval` Ä‘Ã¡nh giÃ¡ thÃ nh pháº§n Ä‘Ã³ dá»±a trÃªn `LLMTestCase` báº¡n táº¡o.
*   Trong vÃ­ dá»¥ nÃ y `AnswerRelevancyMetric()` Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ `inner_component()`.
*   `dataset` chá»‰ Ä‘á»‹nh cÃ¡c **goldens** sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ gá»i `llm_app` cá»§a báº¡n trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡, Ä‘iá»u nÃ y xáº£y ra trong má»™t vÃ²ng láº·p for Ä‘Æ¡n giáº£n.

Khi vÃ²ng láº·p for káº¿t thÃºc, `deepeval` sáº½ tá»•ng há»£p táº¥t cáº£ cÃ¡c sá»‘ liá»‡u, test case trong má»—i thÃ nh pháº§n, vÃ  cháº¡y cÃ¡c Ä‘Ã¡nh giÃ¡ trÃªn táº¥t cáº£ chÃºng, trÆ°á»›c khi táº¡o bÃ¡o cÃ¡o kiá»ƒm thá»­ cuá»‘i cÃ¹ng.

:::info
Khi báº¡n thá»±c hiá»‡n LLM tracing báº±ng `deepeval`, báº¡n cÃ³ thá»ƒ tá»± Ä‘á»™ng Ä‘Ã¡nh giÃ¡ trÃªn **traces, spans, vÃ  threads (cuá»™c há»™i thoáº¡i) trong production**. Chá»‰ cáº§n láº¥y má»™t [khÃ³a API tá»« Confident AI](https://app.confident-ai.com) vÃ  Ä‘áº·t nÃ³ trong CLI:

```bash
CONFIDENT_API_KEY="confident_us..."
```

Viá»‡c triá»ƒn khai LLM tracing cá»§a `deepeval` lÃ  **khÃ´ng xÃ¢m pháº¡m (non-instrusive)**, nghÄ©a lÃ  nÃ³ sáº½ khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n báº¥t ká»³ pháº§n nÃ o trong mÃ£ cá»§a báº¡n.
:::

*   Trace (end-to-end) Evals trong Prod
*   Span (component-level) Evals trong Prod
*   Thread (conversation) Evals trong Prod

ÄÃ¡nh giÃ¡ trÃªn traces lÃ  [cÃ¡c Ä‘Ã¡nh giÃ¡ end-to-end](/docs/evaluation-end-to-end-llm-evals), nÆ¡i má»™t tÆ°Æ¡ng tÃ¡c LLM duy nháº¥t Ä‘ang Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡.

[](https://confident-docs.s3.us-east-1.amazonaws.com/llm-tracing:traces.mp4)

**ÄÃ¡nh giÃ¡ Cáº¥p Ä‘á»™ Trace trong Production**

Spans táº¡o nÃªn má»™t trace vÃ  Ä‘Ã¡nh giÃ¡ trÃªn spans Ä‘áº¡i diá»‡n cho [cÃ¡c Ä‘Ã¡nh giÃ¡ cáº¥p Ä‘á»™ thÃ nh pháº§n](/docs/evaluation-component-level-llm-evals), nÆ¡i cÃ¡c thÃ nh pháº§n riÃªng láº» trong á»©ng dá»¥ng LLM cá»§a báº¡n Ä‘ang Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡.

[](https://confident-docs.s3.us-east-1.amazonaws.com/llm-tracing:spans.mp4)

**ÄÃ¡nh giÃ¡ Cáº¥p Ä‘á»™ Span trong Production**

Threads Ä‘Æ°á»£c táº¡o thÃ nh tá»« **má»™t hoáº·c nhiá»u traces**, vÃ  Ä‘áº¡i diá»‡n cho má»™t tÆ°Æ¡ng tÃ¡c nhiá»u lÆ°á»£t cáº§n Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡.

[](https://confident-docs.s3.us-east-1.amazonaws.com/llm-tracing:threads.mp4)

**ÄÃ¡nh giÃ¡ Thread (há»™i thoáº¡i) trong Production**

## Tiáº¿p Tá»¥c Vá»›i TrÆ°á»ng Há»£p Sá»­ Dá»¥ng Cá»§a Báº¡n

HÃ£y cho chÃºng tÃ´i biáº¿t báº¡n Ä‘ang xÃ¢y dá»±ng cÃ¡i gÃ¬ Ä‘á»ƒ Ä‘Æ°á»£c hÆ°á»›ng dáº«n phÃ¹ há»£p hÆ¡n:

[AI Agents](/docs/getting-started-agents)
*   Thiáº¿t láº­p LLM tracing
*   Kiá»ƒm thá»­ hoÃ n thÃ nh tÃ¡c vá»¥ end-to-end
*   ÄÃ¡nh giÃ¡ cÃ¡c thÃ nh pháº§n riÃªng láº»

[RAG](/docs/getting-started-rag)
*   ÄÃ¡nh giÃ¡ RAG end-to-end
*   Kiá»ƒm thá»­ retriever vÃ  generator riÃªng biá»‡t
*   ÄÃ¡nh giÃ¡ RAG nhiá»u lÆ°á»£t

[Chatbots](/docs/getting-started-chatbots)
*   Thiáº¿t láº­p test case nhiá»u lÆ°á»£t
*   ÄÃ¡nh giÃ¡ cÃ¡c lÆ°á»£t trong má»™t cuá»™c há»™i thoáº¡i
*   MÃ´ phá»ng tÆ°Æ¡ng tÃ¡c ngÆ°á»i dÃ¹ng

*\*Táº¥t cáº£ cÃ¡c hÆ°á»›ng dáº«n báº¯t Ä‘áº§u nhanh Ä‘á»u bao gá»“m hÆ°á»›ng dáº«n vá» cÃ¡ch Ä‘Æ°a Ä‘Ã¡nh giÃ¡ vÃ o production á»Ÿ pháº§n cuá»‘i*

## Hai Cháº¿ Äá»™ ÄÃ¡nh GiÃ¡ LLM

`deepeval` cung cáº¥p hai cháº¿ Ä‘á»™ Ä‘Ã¡nh giÃ¡ chÃ­nh:

[End-to-End LLM Evals](/docs/evaluation-end-to-end-llm-evals)
Tá»‘t nháº¥t cho: Raw LLM APIs, á»©ng dá»¥ng Ä‘Æ¡n giáº£n (khÃ´ng cÃ³ agent), chatbots, vÃ  thá»‰nh thoáº£ng lÃ  RAG.
*   Coi á»©ng dá»¥ng LLM cá»§a báº¡n nhÆ° má»™t há»™p Ä‘en
*   Thiáº¿t láº­p tá»‘i thiá»ƒu, khÃ´ng Ã¡p Ä‘áº·t
*   CÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘Æ°a vÃ o CI/CD
*   Cho má»™t lÆ°á»£t vÃ  nhiá»u lÆ°á»£t

[Component-Level LLM Evals](/docs/evaluation-component-level-llm-evals)
Tá»‘t nháº¥t cho: AI agents, quy trÃ¬nh lÃ m viá»‡c phá»©c táº¡p, Ä‘Ã¡nh giÃ¡ MCP, RAG dá»±a trÃªn thÃ nh pháº§n.
*   Kháº£ nÄƒng hiá»ƒn thá»‹ Ä‘áº§y Ä‘á»§ vÃ o á»©ng dá»¥ng LLM cá»§a báº¡n, kiá»ƒm thá»­ há»™p tráº¯ng
*   Thiáº¿t láº­p LLM tracing khÃ´ng xÃ¢m pháº¡m
*   CÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘Æ°a vÃ o CI/CD
*   Tá»‘t nháº¥t cho má»™t lÆ°á»£t

## TÃ i NguyÃªn Cáº§n Thiáº¿t

ÄÃ¢y lÃ  nhá»¯ng Ä‘iá»u báº¡n cháº¯c cháº¯n nÃªn tÃ¬m hiá»ƒu:

[Metrics](/docs/metrics-introduction)
TÃ¬m hiá»ƒu vá» hÆ¡n 50 sá»‘ liá»‡u cÃ³ sáºµn, cÃ¡ch chá»n vÃ  cÃ¡ch tÃ¹y chá»‰nh chÃºng.

[Datasets](/docs/evaluation-datasets)
TÃ¬m hiá»ƒu cÃ¡ch chÃºng Ä‘Æ°á»£c sá»­ dá»¥ng trong DeepEval, khÃ¡i niá»‡m vá» goldens, vÃ  cÃ¡ch sá»­ dá»¥ng chÃºng cho cÃ¡c Ä‘Ã¡nh giÃ¡.

[Tracing](/docs/evaluation-llm-tracing)
TÃ¬m hiá»ƒu cÃ¡ch trace cÃ¡c á»©ng dá»¥ng LLM cá»§a báº¡n, Ä‘Ã¡nh giÃ¡ á»Ÿ cáº¥p Ä‘á»™ thÃ nh pháº§n, vÃ  giÃ¡m sÃ¡t trong production.

## CÃ¡c Sáº£n Pháº©m KhÃ¡c

TÃ¬m hiá»ƒu thÃªm cÃ¡c dá»‹ch vá»¥ cÃ³ sáºµn trong há»‡ sinh thÃ¡i cá»§a `deepeval`:

[Confident AI](https://www.confident-ai.com/docs)
Ná»n táº£ng Ä‘Ã¡m mÃ¢y cho DeepEval. Cho phÃ©p cáº£ nhÃ³m ká»¹ thuáº­t vÃ  phi ká»¹ thuáº­t cá»™ng tÃ¡c trong viá»‡c kiá»ƒm thá»­ AI, tá»« Ä‘Ã¡nh giÃ¡ trong /dev Ä‘áº¿n /prod.

[DeepTeam](https://trydeepteam.com)
DeepTeam lÃ  DeepEval dÃ nh cho kiá»ƒm thá»­ an toÃ n vÃ  báº£o máº­t AI. PhÆ¡i bÃ y hÆ¡n 50 lá»— há»•ng, vá»›i hÆ¡n 20 phÆ°Æ¡ng phÃ¡p táº¥n cÃ´ng nhÆ° tree jailbreaking, táº¥t cáº£ Ä‘á»u tá»± Ä‘á»™ng.

## VÃ­ Dá»¥ Äáº§y Äá»§

Báº¡n cÃ³ thá»ƒ tÃ¬m tháº¥y vÃ­ dá»¥ Ä‘áº§y Ä‘á»§ [táº¡i Ä‘Ã¢y trÃªn Github cá»§a chÃºng tÃ´i](https://github.com/confident-ai/deepeval/blob/main/examples/getting_started/test_example.py).

[Chá»‰nh sá»­a trang nÃ y](https://github.com/confident-ai/deepeval/edit/main/docs/docs/getting-started.mdx)

Cáº­p nháº­t láº§n cuá»‘i vÃ o **9 thÃ¡ng 1, 2026** bá»Ÿi **Jeffrey Ip**
