# 'Do it yourself' Metrics (Metric T·ª± l√†m)

Trong `deepeval`, b·∫•t k·ª≥ ai c≈©ng c√≥ th·ªÉ d·ªÖ d√†ng x√¢y d·ª±ng metric ƒë√°nh gi√° LLM t√πy ch·ªânh c·ªßa ri√™ng m√¨nh, ƒë∆∞·ª£c t√≠ch h·ª£p t·ª± ƒë·ªông trong h·ªá sinh th√°i c·ªßa `deepeval`, bao g·ªìm:

- Ch·∫°y metric t√πy ch·ªânh c·ªßa b·∫°n trong **CI/CD pipelines**.
- T·∫≠n d·ª•ng c√°c kh·∫£ nƒÉng c·ªßa `deepeval` nh∆∞ **caching metric v√† ƒëa x·ª≠ l√Ω (multi-processing)**.
- K·∫øt qu·∫£ metric t√πy ch·ªânh ƒë∆∞·ª£c **g·ª≠i t·ª± ƒë·ªông ƒë·∫øn Confident AI**.

D∆∞·ªõi ƒë√¢y l√† m·ªôt v√†i l√Ω do t·∫°i sao b·∫°n c√≥ th·ªÉ mu·ªën x√¢y d·ª±ng metric ƒë√°nh gi√° LLM c·ªßa ri√™ng m√¨nh:

- **B·∫°n mu·ªën ki·ªÉm so√°t t·ªët h∆°n** c√°c ti√™u ch√≠ ƒë√°nh gi√° ƒë∆∞·ª£c s·ª≠ d·ª•ng (v√† b·∫°n nghƒ© r·∫±ng [`GEval`](/docs/metrics-llm-evals) ho·∫∑c [`DAG`](/docs/metrics-dag) l√† kh√¥ng ƒë·ªß).
- **B·∫°n kh√¥ng mu·ªën s·ª≠ d·ª•ng LLM** ƒë·ªÉ ƒë√°nh gi√° (v√¨ t·∫•t c·∫£ c√°c metric trong `deepeval` ƒë·ªÅu ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi LLM).
- **B·∫°n mu·ªën k·∫øt h·ª£p nhi·ªÅu metric c·ªßa `deepeval`** (v√≠ d·ª•: s·∫Ω r·∫•t h·ª£p l√Ω khi c√≥ m·ªôt metric ki·ªÉm tra c·∫£ s·ª± li√™n quan c·ªßa c√¢u tr·∫£ l·ªùi v√† t√≠nh trung th·ª±c).

:::info
C√≥ nhi·ªÅu c√°ch ƒë·ªÉ tri·ªÉn khai m·ªôt metric ƒë√°nh gi√° LLM. ƒê√¢y l√† m·ªôt [b√†i vi·∫øt tuy·ªát v·ªùi v·ªÅ m·ªçi th·ª© b·∫°n c·∫ßn bi·∫øt v·ªÅ vi·ªác ch·∫•m ƒëi·ªÉm c√°c metric ƒë√°nh gi√° LLM.](https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation)
:::

## C√°c Quy t·∫Øc C·∫ßn Tu√¢n th·ªß Khi T·∫°o Metric T√πy ch·ªânh

### 1. K·∫ø th·ª´a l·ªõp `BaseMetric`

ƒê·ªÉ b·∫Øt ƒë·∫ßu, h√£y t·∫°o m·ªôt l·ªõp k·∫ø th·ª´a t·ª´ l·ªõp `BaseMetric` c·ªßa `deepeval`:

```python
from deepeval.metrics import BaseMetric  
  
class CustomMetric(BaseMetric):  
    ...
```

ƒêi·ªÅu n√†y r·∫•t quan tr·ªçng v√¨ l·ªõp `BaseMetric` s·∫Ω gi√∫p `deepeval` nh·∫≠n ra metric t√πy ch·ªânh c·ªßa b·∫°n trong qu√° tr√¨nh ƒë√°nh gi√°.

### 2. Tri·ªÉn khai ph∆∞∆°ng th·ª©c `__init__()`

L·ªõp `BaseMetric` cung c·∫•p cho metric t√πy ch·ªânh c·ªßa b·∫°n m·ªôt v√†i thu·ªôc t√≠nh m√† b·∫°n c√≥ th·ªÉ c·∫•u h√¨nh v√† hi·ªÉn th·ªã sau khi ƒë√°nh gi√°, ho·∫∑c c·ª•c b·ªô ho·∫∑c tr√™n Confident AI.

M·ªôt v√≠ d·ª• l√† thu·ªôc t√≠nh `threshold` (ng∆∞·ª°ng), x√°c ƒë·ªãnh xem `LLMTestCase` ƒëang ƒë∆∞·ª£c ƒë√°nh gi√° c√≥ ƒë·∫°t hay kh√¥ng. M·∫∑c d√π **thu·ªôc t√≠nh `threshold` l√† t·∫•t c·∫£ nh·ªØng g√¨ b·∫°n c·∫ßn ƒë·ªÉ l√†m cho m·ªôt metric t√πy ch·ªânh ho·∫°t ƒë·ªông**, ƒë√¢y l√† m·ªôt s·ªë thu·ªôc t√≠nh b·ªï sung cho nh·ªØng ng∆∞·ªùi mu·ªën t√πy ch·ªânh nhi·ªÅu h∆°n:

- `evaluation_model`: m·ªôt `str` ch·ªâ ƒë·ªãnh t√™n c·ªßa m√¥ h√¨nh ƒë√°nh gi√° ƒë∆∞·ª£c s·ª≠ d·ª•ng.
- `include_reason`: m·ªôt `bool` ch·ªâ ƒë·ªãnh xem c√≥ bao g·ªìm l√Ω do c√πng v·ªõi ƒëi·ªÉm s·ªë metric hay kh√¥ng. ƒêi·ªÅu n√†y s·∫Ω kh√¥ng c·∫ßn thi·∫øt n·∫øu b·∫°n kh√¥ng c√≥ √Ω ƒë·ªãnh s·ª≠ d·ª•ng LLM ƒë·ªÉ ƒë√°nh gi√°.
- `strict_mode`: m·ªôt `bool` ch·ªâ ƒë·ªãnh xem c√≥ ch·ªâ th√¥ng qua metric n·∫øu c√≥ ƒëi·ªÉm s·ªë ho√†n h·∫£o hay kh√¥ng.
- `async_mode`: m·ªôt `bool` ch·ªâ ƒë·ªãnh xem c√≥ th·ª±c thi metric b·∫•t ƒë·ªìng b·ªô hay kh√¥ng.

:::tip
ƒê·ª´ng qu√° b·∫≠n t√¢m v√†o c√°c thu·ªôc t√≠nh n√¢ng cao ngay b√¢y gi·ªù, ch√∫ng ta s·∫Ω xem x√©t c√°ch ch√∫ng c√≥ th·ªÉ h·ªØu √≠ch trong c√°c ph·∫ßn sau c·ªßa h∆∞·ªõng d·∫´n n√†y.
:::

Ph∆∞∆°ng th·ª©c `__init__()` l√† n∆°i tuy·ªát v·ªùi ƒë·ªÉ thi·∫øt l·∫≠p c√°c thu·ªôc t√≠nh n√†y:

```python
from deepeval.metrics import BaseMetric  
  
class CustomMetric(BaseMetric):  
    def __init__(  
        self,  
        threshold: float = 0.5,  
        # Optional  
        evaluation_model: str,  
        include_reason: bool = True,  
        strict_mode: bool = True,  
        async_mode: bool = True  
    ):  
        self.threshold = threshold  
        # Optional  
        self.evaluation_model = evaluation_model  
        self.include_reason = include_reason  
        self.strict_mode = strict_mode  
        self.async_mode = async_mode
```

### 3. Tri·ªÉn khai c√°c ph∆∞∆°ng th·ª©c `measure()` v√† `a_measure()`

Ph∆∞∆°ng th·ª©c `measure()` v√† `a_measure()` l√† n∆°i t·∫•t c·∫£ qu√° tr√¨nh ƒë√°nh gi√° di·ªÖn ra. Trong `deepeval`, ƒë√°nh gi√° l√† qu√° tr√¨nh √°p d·ª•ng m·ªôt metric v√†o m·ªôt `LLMTestCase` ƒë·ªÉ t·∫°o ra ƒëi·ªÉm s·ªë v√† t√πy ch·ªçn m·ªôt l√Ω do cho ƒëi·ªÉm s·ªë (n·∫øu b·∫°n ƒëang s·ª≠ d·ª•ng LLM) d·ª±a tr√™n thu·∫≠t to√°n ch·∫•m ƒëi·ªÉm.

Ph∆∞∆°ng th·ª©c `a_measure()` ƒë∆°n gi·∫£n l√† tri·ªÉn khai b·∫•t ƒë·ªìng b·ªô c·ªßa ph∆∞∆°ng th·ª©c `measure()`, v√† do ƒë√≥ c·∫£ hai n√™n s·ª≠ d·ª•ng c√πng m·ªôt thu·∫≠t to√°n ch·∫•m ƒëi·ªÉm.

:::info
Ph∆∞∆°ng th·ª©c `a_measure()` cho ph√©p `deepeval` ch·∫°y metric t√πy ch·ªânh c·ªßa b·∫°n m·ªôt c√°ch b·∫•t ƒë·ªìng b·ªô. L·∫•y h√†m `assert_test` l√†m v√≠ d·ª•:

```python
from deepeval import assert_test  
  
def test_multiple_metrics():  
    ...  
    assert_test(test_case, [metric1, metric2], run_async=True)
```

Khi b·∫°n ch·∫°y `assert_test()` v·ªõi `run_async=True` (ƒë√¢y l√† h√†nh vi m·∫∑c ƒë·ªãnh), `deepeval` g·ªçi ph∆∞∆°ng th·ª©c `a_measure()` cho ph√©p t·∫•t c·∫£ c√°c metric ch·∫°y ƒë·ªìng th·ªùi theo c√°ch kh√¥ng ch·∫∑n (non-blocking).
:::

C·∫£ `measure()` v√† `a_measure()` **PH·∫¢I**:

- ch·∫•p nh·∫≠n m·ªôt `LLMTestCase` l√†m ƒë·ªëi s·ªë
- thi·∫øt l·∫≠p `self.score`
- thi·∫øt l·∫≠p `self.success`

B·∫°n c≈©ng c√≥ th·ªÉ t√πy ch·ªçn thi·∫øt l·∫≠p `self.reason` trong c√°c ph∆∞∆°ng th·ª©c ƒëo l∆∞·ªùng (n·∫øu b·∫°n ƒëang s·ª≠ d·ª•ng LLM ƒë·ªÉ ƒë√°nh gi√°), ho·∫∑c b·ªçc m·ªçi th·ª© trong m·ªôt kh·ªëi `try` ƒë·ªÉ b·∫Øt b·∫•t k·ª≥ ngo·∫°i l·ªá n√†o v√† thi·∫øt l·∫≠p n√≥ cho `self.error`. D∆∞·ªõi ƒë√¢y l√† m·ªôt v√≠ d·ª• gi·∫£ ƒë·ªãnh:

```python
from deepeval.metrics import BaseMetric  
from deepeval.test_case import LLMTestCase  
  
class CustomMetric(BaseMetric):  
    ...  
  
    def measure(self, test_case: LLMTestCase) -> float:  
        # Although not required, we recommend catching errors  
        # in a try block  
        try:  
            self.score = generate_hypothetical_score(test_case)  
            if self.include_reason:  
                self.reason = generate_hypothetical_reason(test_case)  
            self.success = self.score >= self.threshold  
            return self.score  
        except Exception as e:  
            # set metric error and re-raise it  
            self.error = str(e)  
            raise  
  
    async def a_measure(self, test_case: LLMTestCase) -> float:  
        # Although not required, we recommend catching errors  
        # in a try block  
        try:  
            self.score = await async_generate_hypothetical_score(test_case)  
            if self.include_reason:  
                self.reason = await async_generate_hypothetical_reason(test_case)  
            self.success = self.score >= self.threshold  
            return self.score  
        except Exception as e:  
            # set metric error and re-raise it  
            self.error = str(e)  
            raise
```

:::tip
Th∆∞·ªùng th√¨, ph·∫ßn g√¢y ch·∫∑n (blocking) c·ªßa m·ªôt metric ƒë√°nh gi√° LLM b·∫Øt ngu·ªìn t·ª´ c√°c l·ªánh g·ªçi API ƒë·∫øn nh√† cung c·∫•p LLM c·ªßa b·∫°n (ch·∫≥ng h·∫°n nh∆∞ c√°c ƒëi·ªÉm cu·ªëi API c·ªßa OpenAI), v√† do ƒë√≥ cu·ªëi c√πng b·∫°n s·∫Ω ph·∫£i ƒë·∫£m b·∫£o r·∫±ng vi·ªác suy lu·∫≠n LLM th·ª±c s·ª± c√≥ th·ªÉ ƒë∆∞·ª£c th·ª±c hi·ªán b·∫•t ƒë·ªìng b·ªô.

N·∫øu b·∫°n ƒë√£ kh√°m ph√° t·∫•t c·∫£ c√°c l·ª±a ch·ªçn v√† nh·∫≠n ra kh√¥ng c√≥ tri·ªÉn khai b·∫•t ƒë·ªìng b·ªô n√†o cho cu·ªôc g·ªçi LLM c·ªßa b·∫°n (v√≠ d·ª•: n·∫øu b·∫°n ƒëang s·ª≠ d·ª•ng m√¥ h√¨nh m√£ ngu·ªìn m·ªü t·ª´ th∆∞ vi·ªán `transformers` c·ªßa Hugging Face), ƒë∆°n gi·∫£n l√† **t√°i s·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c `measure` trong `a_measure()`**:

```python
from deepeval.metrics import BaseMetric  
from deepeval.test_case import LLMTestCase  
  
class CustomMetric(BaseMetric):  
    ...  
  
    async def a_measure(self, test_case: LLMTestCase) -> float:  
        return self.measure(test_case)
```

B·∫°n c≈©ng c√≥ th·ªÉ [nh·∫•n v√†o ƒë√¢y ƒë·ªÉ t√¨m m·ªôt v√≠ d·ª• v·ªÅ vi·ªác gi·∫£m t·∫£i suy lu·∫≠n LLM sang m·ªôt lu·ªìng ri√™ng bi·ªát](/docs/metrics-introduction#mistral-7b-example) nh∆∞ m·ªôt gi·∫£i ph√°p thay th·∫ø, m·∫∑c d√π n√≥ c√≥ th·ªÉ kh√¥ng ho·∫°t ƒë·ªông cho t·∫•t c·∫£ c√°c tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng.
:::

### 4. Tri·ªÉn khai ph∆∞∆°ng th·ª©c `is_successful()`

V·ªÅ c∆° b·∫£n, `deepeval` g·ªçi ph∆∞∆°ng th·ª©c `is_successful()` ƒë·ªÉ x√°c ƒë·ªãnh tr·∫°ng th√°i c·ªßa metric c·ªßa b·∫°n cho m·ªôt `LLMTestCase` nh·∫•t ƒë·ªãnh. H·ªá th·ªëng khuy√™n b·∫°n n√™n sao ch√©p v√† d√°n tr·ª±c ti·∫øp ƒëo·∫°n m√£ d∆∞·ªõi ƒë√¢y l√†m tri·ªÉn khai `is_successful()` c·ªßa b·∫°n:

```python
from deepeval.metrics import BaseMetric  
from deepeval.test_case import LLMTestCase  
  
class CustomMetric(BaseMetric):  
    ...  
  
    def is_successful(self) -> bool:  
        if self.error is not None:  
            self.success = False  
        else:  
            return self.success
```

### 5. ƒê·∫∑t t√™n cho Metric T√πy ch·ªânh c·ªßa B·∫°n

C√≥ l·∫Ω l√† b∆∞·ªõc d·ªÖ nh·∫•t, t·∫•t c·∫£ nh·ªØng g√¨ c√≤n l·∫°i l√† ƒë·∫∑t t√™n cho metric t√πy ch·ªânh c·ªßa b·∫°n:

```python
from deepeval.metrics import BaseMetric  
from deepeval.test_case import LLMTestCase  
  
class CustomMetric(BaseMetric):  
    ...  
  
    @property  
    def __name__(self):  
        return "My Custom Metric"
```

**Ch√∫c m·ª´ng üéâ!** B·∫°n v·ª´a h·ªçc c√°ch x√¢y d·ª±ng m·ªôt metric t√πy ch·ªânh ƒë∆∞·ª£c t√≠ch h·ª£p 100% v·ªõi h·ªá sinh th√°i c·ªßa `deepeval`. Trong ph·∫ßn ti·∫øp theo, ch√∫ng ta s·∫Ω ƒëi qua m·ªôt v√†i v√≠ d·ª• th·ª±c t·∫ø.

## Th√™m V√≠ d·ª•

### Non-LLM Evals (ƒê√°nh gi√° kh√¥ng d√πng LLM)

LLM-Eval l√† m·ªôt metric ƒë√°nh gi√° LLM ƒë∆∞·ª£c ch·∫•m ƒëi·ªÉm b·∫±ng c√°ch s·ª≠ d·ª•ng m·ªôt LLM, v√† do ƒë√≥ non-LLM eval ƒë∆°n gi·∫£n l√† m·ªôt metric kh√¥ng ƒë∆∞·ª£c ch·∫•m ƒëi·ªÉm b·∫±ng c√°ch s·ª≠ d·ª•ng LLM. Trong v√≠ d·ª• n√†y, ch√∫ng t√¥i s·∫Ω minh h·ªça c√°ch s·ª≠ d·ª•ng [rouge score](https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation) thay th·∫ø:

```python
from deepeval.scorer import Scorer  
from deepeval.metrics import BaseMetric  
from deepeval.test_case import LLMTestCase  
  
class RougeMetric(BaseMetric):  
    def __init__(self, threshold: float = 0.5):  
        self.threshold = threshold  
        self.scorer = Scorer()  
  
    def measure(self, test_case: LLMTestCase):  
        self.score = self.scorer.rouge_score(  
            prediction=test_case.actual_output,  
            target=test_case.expected_output,  
            score_type="rouge1"  
        )  
        self.success = self.score >= self.threshold  
        return self.score  
  
    # Async implementation of measure(). If async version for  
    # scoring method does not exist, just reuse the measure method.  
    async def a_measure(self, test_case: LLMTestCase):  
        return self.measure(test_case)  
  
    def is_successful(self):  
        return self.success  
  
    @property  
    def __name__(self):  
        return "Rouge Metric"
```

:::note
M·∫∑c d√π b·∫°n ƒë∆∞·ª£c t·ª± do tri·ªÉn khai rouge scorer c·ªßa ri√™ng m√¨nh, b·∫°n s·∫Ω nh·∫≠n th·∫•y r·∫±ng d√π kh√¥ng ƒë∆∞·ª£c ghi trong t√†i li·ªáu, `deepeval` cung c·∫•p th√™m m·ªôt module `scorer` cho ph∆∞∆°ng ph√°p ch·∫•m ƒëi·ªÉm NLP truy·ªÅn th·ªëng h∆°n v√† c√≥ th·ªÉ ƒë∆∞·ª£c t√¨m th·∫•y [t·∫°i ƒë√¢y.](https://github.com/confident-ai/deepeval/blob/main/deepeval/scorer/scorer.py)

H√£y ch·∫Øc ch·∫Øn ch·∫°y `pip install rouge-score` n·∫øu `rouge-score` ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t trong m√¥i tr∆∞·ªùng c·ªßa b·∫°n.
:::

B√¢y gi·ªù b·∫°n c√≥ th·ªÉ ch·∫°y metric t√πy ch·ªânh n√†y nh∆∞ m·ªôt metric ƒë·ªôc l·∫≠p ch·ªâ trong v√†i d√≤ng m√£:

```python
...  
  
#####################  
### Example Usage ###  
#####################  
test_case = LLMTestCase(input="...", actual_output="...", expected_output="...")  
metric = RougeMetric()  
  
metric.measure(test_case)  
print(metric.is_successful())
```

### Composite Metrics (Metric H·ªón h·ª£p)

Trong v√≠ d·ª• n√†y, ch√∫ng t√¥i s·∫Ω k·∫øt h·ª£p hai metric m·∫∑c ƒë·ªãnh c·ªßa `deepeval` th√†nh metric t√πy ch·ªânh c·ªßa ch√∫ng t√¥i, do ƒë√≥ t·∫°i sao ch√∫ng t√¥i g·ªçi n√≥ l√† metric "h·ªón h·ª£p" (composite).

H·ªá th·ªëng s·∫Ω k·∫øt h·ª£p `AnswerRelevancyMetric` v√† `FaithfulnessMetric`, v√¨ ch√∫ng t√¥i hi·∫øm khi th·∫•y ng∆∞·ªùi d√πng n√†o quan t√¢m ƒë·∫øn c√°i n√†y m√† kh√¥ng quan t√¢m ƒë·∫øn c√°i kia.

```python
from deepeval.metrics import BaseMetric, AnswerRelevancyMetric, FaithfulnessMetric  
from deepeval.test_case import LLMTestCase  
  
class FaithfulRelevancyMetric(BaseMetric):  
    def __init__(  
        self,  
        threshold: float = 0.5,  
        evaluation_model: Optional[str] = "gpt-4-turbo",  
        include_reason: bool = True,  
        async_mode: bool = True,  
        strict_mode: bool = False,  
    ):  
        self.threshold = 1 if strict_mode else threshold  
        self.evaluation_model = evaluation_model  
        self.include_reason = include_reason  
        self.async_mode = async_mode  
        self.strict_mode = strict_mode  
  
    def measure(self, test_case: LLMTestCase):  
        try:  
            relevancy_metric, faithfulness_metric = initialize_metrics()  
            # Remember, deepeval's default metrics follow the same pattern as your custom metric!  
            relevancy_metric.measure(test_case)  
            faithfulness_metric.measure(test_case)  
  
            # Custom logic to set score, reason, and success  
            set_score_reason_success(relevancy_metric, faithfulness_metric)  
            return self.score  
        except Exception as e:  
            # Set and re-raise error  
            self.error = str(e)  
            raise  
  
    async def a_measure(self, test_case: LLMTestCase):  
        try:  
            relevancy_metric, faithfulness_metric = initialize_metrics()  
            # Here, we use the a_measure() method instead so both metrics can run concurrently  
            await relevancy_metric.a_measure(test_case)  
            await faithfulness_metric.a_measure(test_case)  
  
            # Custom logic to set score, reason, and success  
            set_score_reason_success(relevancy_metric, faithfulness_metric)  
            return self.score  
        except Exception as e:  
            # Set and re-raise error  
            self.error = str(e)  
            raise  
  
    def is_successful(self) -> bool:  
        if self.error is not None:  
            self.success = False  
        else:  
            return self.success  
  
    @property  
    def __name__(self):  
        return "Composite Relevancy Faithfulness Metric"  
  
  
    ######################  
    ### Helper methods ###  
    ######################  
    def initialize_metrics(self):  
        relevancy_metric = AnswerRelevancyMetric(  
            threshold=self.threshold,  
            model=self.evaluation_model,  
            include_reason=self.include_reason,  
            async_mode=self.async_mode,  
            strict_mode=self.strict_mode  
        )  
        faithfulness_metric = FaithfulnessMetric(  
            threshold=self.threshold,  
            model=self.evaluation_model,  
            include_reason=self.include_reason,  
            async_mode=self.async_mode,  
            strict_mode=self.strict_mode  
        )  
        return relevancy_metric, faithfulness_metric  
  
    def set_score_reason_success(  
        self,  
        relevancy_metric: BaseMetric,  
        faithfulness_metric: BaseMetric  
    ):  
        # Get scores and reasons for both  
        relevancy_score = relevancy_metric.score  
        relevancy_reason = relevancy_metric.reason  
        faithfulness_score = faithfulness_metric.score  
        faithfulness_reason = faithfulness_reason.reason  
  
        # Custom logic to set score  
        composite_score = min(relevancy_score, faithfulness_score)  
        self.score = 0 if self.strict_mode and composite_score < self.threshold else composite_score  
  
        # Custom logic to set reason  
        if include_reason:  
            self.reason = relevancy_reason + "\n" + faithfulness_reason  
  
        # Custom logic to set success  
        self.success = self.score >= self.threshold
```

B√¢y gi·ªù h√£y th·ª≠ s·ª≠ d·ª•ng n√≥:

`test_llm.py`

```python
from deepeval import assert_test  
from deepeval.test_case import LLMTestCase  
...  
  
def test_llm():  
    metric = FaithfulRelevancyMetric()  
    test_case = LLMTestCase(...)  
    assert_test(test_case, [metric])
```

```bash
deepeval test run test_llm.py
```
