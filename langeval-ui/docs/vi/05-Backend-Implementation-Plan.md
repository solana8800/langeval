# BACKEND IMPLEMENTATION SPECIFICATION
**Project**: Enterprise AI Agent Evaluation Platform
**Status**: STABLE

## 1. System Overview & Microservices Architecture

D·ª±a v√†o **[System-Architecture](./01-System-Architecture.md)**, h·ªá th·ªëng Backend ƒë∆∞·ª£c chia th√†nh c√°c Microservices sau:

| Service Name | Technology Stack | Responsibility | UI Module Mapping |
| :--- | :--- | :--- | :--- |
| **Identity Service** | Python (FastAPI) + Google OAuth 2.0 | User Management, JWT Auth, Workspace RBAC, Auto-provisioning. | `settings/team`, `profile` |
| **Resource Service** | Python (FastAPI) + PostgreSQL | CRUD cho Agents (Encrypted), Scenarios, Models, KB, Metrics. H·ªó tr·ª£ Pagination. | `agents`, `models`, `knowledge-bases` |
| **Orchestrator Service** | Python (LangGraph) + Redis | ƒêi·ªÅu ph·ªëi Campaign, Dynamic Graph Building, State Persistence. | `scenario-builder`, `battle-arena` |
| **Simulation Worker** | Python (AutoGen) | Ch·∫°y gi·∫£ l·∫≠p h·ªôi tho·∫°i (Multi-turn), Red Teaming (Adversarial). | `battle-arena` (Sim side) |
| **Evaluation Worker** | Python (DeepEval) | Ch·∫•m ƒëi·ªÉm LLM-as-a-judge, Confidence Threshold, Human-in-the-loop. | `metrics-library`, `reports` |
| **Data Ingestion** | **Rust** (Actix-web) | Thu th·∫≠p Log/Trace t·ªëc ƒë·ªô cao, Batching d·ªØ li·ªáu v√†o ClickHouse. | `trace`, `dev-console` |
| **GenAI Service** | Python (LangChain) | Sinh Persona, Test Cases, Goldens t·ª´ PDF (RAG), Prompt Optimization (GEPA). | `dataset-gen`, `prompt-optimizer` |
| **Langfuse** | Infrastructure (Docker) | Tracing, Observability, evaluation Store. | `trace` |
| **Clickhouse** | Infrastructure (Docker) | Ph√¢n t√≠ch Log/Trace hi·ªáu nƒÉng cao. | `trace`, `reports` |
| **Kafka + Zookeeper** | Infrastructure (Docker) | Event Streaming, Async Tasks (Simulation/Evaluation). | N/A (Backend Internal) |
| **Redis** | Infrastructure (Docker) | Caching, LangGraph Checkpointer (Persistence). Qu·∫£n l√Ω Rate Limit & API Usage Limit. | N/A (Backend Internal) |
| **Billing Service** | Python (FastAPI) | T√≠ch h·ª£p PayPal, qu·∫£n l√Ω Subscriptions (G√≥i c∆∞·ªõc, Quota Limit Tracking). | `billing`, `pricing` |

---

---

## 2. The Quad-Core Technology Stack

H·ªá th·ªëng ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n 4 tr·ª• c·ªôt c√¥ng ngh·ªá (Quad-Core) ƒë·ªÉ ƒë·∫£m b·∫£o kh·∫£ nƒÉng m·ªü r·ªông, ƒë·ªô s√¢u ki·ªÉm th·ª≠ v√† t√≠nh an to√†n d·ªØ li·ªáu.

### 2.1. Orchestration: LangGraph
*   **Role**: ƒêi·ªÅu ph·ªëi lu·ªìng ki·ªÉm th·ª≠ ph·ª©c t·∫°p, h·ªó tr·ª£ Cyclic Graphs (v√≤ng l·∫∑p t·ª± s·ª≠a sai).
*   **Deep Integration**:
    *   S·ª≠ d·ª•ng `StateGraph` ƒë·ªÉ ƒë·ªãnh nghƒ©a quy tr√¨nh: `User Sim -> Target Bot -> Evaluator -> Decision -> Loop/End`.
    *   Qu·∫£n l√Ω State b·ªÅn v·ªØng (Persistence) qua Redis, cho ph√©p "Time Travel" ƒë·ªÉ debug t·ª´ng b∆∞·ªõc nh·∫£y c·ªßa Agent.

### 2.2. Simulation: Microsoft AutoGen
*   **Role**: Gi·∫£ l·∫≠p ng∆∞·ªùi d√πng (User Simulator) v·ªõi c√°c t√≠nh c√°ch (Persona) kh√°c nhau.
*   **Why AutoGen?**:
    *   **Conversable Agents**: Thi·∫øt k·∫ø t·ªëi ∆∞u cho h·ªôi tho·∫°i nhi·ªÅu chi·ªÅu (Multi-turn), t·ª± nhi√™n h∆°n so v·ªõi c∆° ch·∫ø Chain ƒë∆°n gi·∫£n.
    *   **Headless Mode**: Ch·∫°y ng·∫ßm trong Docker Worker, kh√¥ng c·∫ßn giao di·ªán AutoGen Studio.
    *   **Human Proxy**: C√≥ kh·∫£ nƒÉng cho ph√©p QA can thi·ªáp th·ªß c√¥ng (Human-in-the-loop) khi c·∫ßn thi·∫øt.

### 2.3. Evaluation: DeepEval
*   **Role**: Ch·∫•m ƒëi·ªÉm (LLM-as-a-Judge) chuy√™n s√¢u cho RAG v√† Agents.
*   **Key Features**:
    *   **G-Eval**: T·∫°o metrics t√πy ch·ªânh d·ª±a tr√™n Prompt.
    *   **Synthesizer**: T·ª± sinh d·ªØ li·ªáu test (Synthetic Data) t·ª´ t√†i li·ªáu nghi·ªáp v·ª•.
    *   **Unit Test Integration**: Vi·∫øt test case nh∆∞ PyTest, t√≠ch h·ª£p th·∫≥ng v√†o CI/CD pipeline.

### 2.4. Observability: Langfuse
*   **Role**: Quan s√°t, truy v·∫øt (Tracing) v√† debugging.
*   **Data Sovereignty**: Deployment Self-hosted (Docker) ƒë·∫£m b·∫£o kh√¥ng l·ªçt d·ªØ li·ªáu nh·∫°y c·∫£m ra ngo√†i (Privacy-first).
*   **Trace Linking**: G·∫Øn k·∫øt ch·∫∑t ch·∫Ω Trace ID xuy√™n su·ªët t·ª´ AutoGen -> LangGraph -> Target Bot.
*   **Future Strategy (Arize Phoenix)**:
    *   H·ªá th·ªëng ƒë∆∞·ª£c thi·∫øt k·∫ø v·ªõi **Thin Wrapper SDK** (`langeval-sdk`).
    *   Gi√∫p d·ªÖ d√†ng chuy·ªÉn ƒë·ªïi Backend t·ª´ Langfuse sang **Arize Phoenix** (ƒë·ªÉ ph√¢n t√≠ch chuy√™n s√¢u Embedding/Drift) m√† **KH√îNG** c·∫ßn s·ª≠a ƒë·ªïi code c·ªßa Agent. Ch·ªâ c·∫ßn c·∫≠p nh·∫≠t logic b√™n trong SDK.

### 2.5. Dual-Flow Data Pipeline Architecture

H·ªá th·ªëng backend ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a ƒë·ªÉ x·ª≠ l√Ω 2 lu·ªìng d·ªØ li·ªáu ri√™ng bi·ªát:

**Flow A: Active Evaluation (Scenario-driven)**
*   **Path**: `User UI -> Orchestrator -> Kafka (Simulation) -> AutoGen Worker -> Kafka (Evaluation) -> DeepEval Worker -> Redis/DB`.
*   **Characteristics**:
    *   **Sequential**: C√°c b∆∞·ªõc ch·∫°y tu·∫ßn t·ª±, c√≥ dependency (Step 2 ch·ªù Step 1).
    *   **Stateful**: C·∫ßn gi·ªØ tr·∫°ng th√°i h·ªôi tho·∫°i (Context) ch√©o gi·ªØa c√°c worker.
    *   **High Latency**: Ch·∫•p nh·∫≠n ƒë·ªô tr·ªÖ (v√†i gi√¢y ƒë·∫øn ph√∫t) ƒë·ªÉ x·ª≠ l√Ω logic ph·ª©c t·∫°p.

**Flow B: Passive Monitoring (Trace-driven)**
*   **Path**: `Bot SDK/API -> Data Ingestion Service (Rust) -> Kafka (Traces) -> ClickHouse`.
*   **Characteristics**:
    *   **Fire-and-forget**: SDK g·ª≠i log v√† kh√¥ng c·∫ßn ch·ªù ph·∫£n h·ªìi ngay.
    *   **High Throughput**: X·ª≠ l√Ω h√†ng ngh√¨n logs/s.
    *   **Sampling Evaluation**: M·ªôt worker ri√™ng (`Evaluation Worker`) s·∫Ω subscribe v√†o Kafka Topic `traces`, l·∫•y m·∫´u ng·∫´u nhi√™n (vd: 10%) ƒë·ªÉ ch·∫•m ƒëi·ªÉm ng·∫ßm, kh√¥ng ·∫£nh h∆∞·ªüng lu·ªìng ch√≠nh.

---

### 2.5.1 Scoring Logic Diagrams

Chi ti·∫øt v·ªÅ lu·ªìng x·ª≠ l√Ω E2E t·ª´ khi g·ªçi API ƒë·∫øn khi c√≥ k·∫øt qu·∫£:

**Sequence Diagram: Full End-to-End Evaluation Flow**

```mermaid
sequenceDiagram
    autonumber
    participant C as Client (UI/Script)
    participant API as Orchestrator API
    participant BG as Orchestrator Worker (LangGraph)
    participant S as Simulation Worker
    participant T as Target Agent
    participant E as Evaluation Worker
    participant LLM as LLM Judge (DeepEval)
    participant DB as Redis/DB

    Note over C, DB: Full End-to-End Evaluation Flow

    C->>API: POST /campaigns (Scenario + Agent)
    activate API
    API->>DB: Create Campaign (Status: queued)
    API->>BG: Trigger Background Task
    API-->>C: Return Campaign ID (201 Created)
    deactivate API

    activate BG
    BG->>DB: Update Status -> running
    BG->>BG: Build Dynamic Graph (Nodes/Edges)
    
    loop For Each Node (Simulation)
        BG->>S: Kafka: Send Task Payload
        activate S
        Note over S: User Simulator (AutoGen)
        S->>S: Generate User Message (LLM/Persona)
        S->>T: Send User Message
        activate T
        T-->>S: Return Response
        deactivate T
        S->>S: Calculate response_time
        S-->>BG: Redis: Return Result + Metrics
        deactivate S
        
        BG->>BG: Update State (messages, metrics)
    end

    loop For Each Expectation (Evaluation)
        BG->>E: Kafka: Send Expectation Payload
        activate E
        E->>E: Check Provider (Basic vs DeepEval)
        
        alt Basic Provider
            E->>E: Check Basic Match (keywords)
            E->>E: Check Politeness (heuristic)
        else DeepEval Provider
            E->>E: MetricFactory: Create Metrics
            E->>LLM: Evaluate (AnswerRelevancy, etc.)
            LLM-->>E: Return Score + Reason
        end
        
        E-->>BG: Redis: Return Result + Metrics
        deactivate E
        
        BG->>BG: Aggregate Scores (raw_score += node_score)
    end

    BG->>BG: Normalize Score (0-10 Scale)
    BG->>DB: Persist Final State (Status: completed)
    deactivate BG
```

**Flow Diagram: System Process**

```mermaid
flowchart TD
    subgraph API Layer
        Req([POST /campaigns]) --> Validate[Validate Request]
        Validate --> SaveDB[(Save to DB: queued)]
        SaveDB --> TrigBG[Trigger Background Task]
        TrigBG --> Resp([Return Campaign ID])
    end

    TrigBG -.-> |Async| StartGraph
    
    subgraph Orchestrator Logic
        StartGraph([Start Workflow]) --> InitState[Init State: running]
        InitState --> BuildGraph[Build LangGraph]
        BuildGraph --> SimNode[Simulation Node]
    end
    
    subgraph Simulation Worker
        SimNode --> |Kafka| ConsSim[Consumer: Exec Task]
        ConsSim --> GenMsg["Generate User Message (LLM)"]
        GenMsg --> CallAgent[Call Target Agent]
        CallAgent --> MeasureTime[Measure Response Time]
        MeasureTime --> |Redis| ReturnSim[Return Output]
    end
    
    ReturnSim --> MergeSim[Merge Metrics to State]
    MergeSim --> ExpNode[Expectation Node]
    
    subgraph Evaluation Worker
        ExpNode --> |Kafka| ConsEval[Consumer: Evaluate]
        ConsEval --> CheckMode{Provider?}
        
        CheckMode -- Basic --> CheckKeywords[Check Keywords]
        CheckKeywords --> CalcPolite["Calculate Politeness (Heuristic)"]
        CheckKeywords --> CalcMatch["Calculate Basic Match"]
        CalcMatch --> ReturnEval
        
        CheckMode -- DeepEval --> LoadMetrics[MetricFactory: Load Metrics]
        LoadMetrics --> CallLLM[Call LLM Judge]
        CallLLM --> CalcDeep["Calculate DeepEval Score"]
        CalcDeep --> ReturnEval[Return Metrics]
    end
    
    ReturnEval --> AggScore[Aggregator: raw_score += node_score]
    AggScore --> CheckMore{More Nodes?}
    CheckMore -- Yes --> SimNode
    CheckMore -- No --> EndNode[End Node]
    
    EndNode --> Normalize["Normalize: (raw_score / exp_count) * 10"]
    Normalize --> FinalDB[(Update DB: completed)]
    FinalDB --> End([End Workflow])
```

---

### 2.6. Specialized Implementation Details

C√°ch ki·∫øn tr√∫c Quad-Core x·ª≠ l√Ω c√°c k·ªãch b·∫£n ƒë·∫∑c bi·ªát:

| Scenario | Orchestrator Logic | Simulation Config | Evaluation Strategy |
| :--- | :--- | :--- | :--- |
| **Battle Arena** | Fork lu·ªìng: G·ª≠i 1 input t·ªõi 2 Target Bots song song. Sync ch·ªù c·∫£ 2 tr·∫£ v·ªÅ. | `HumanProxy` t·∫Øt. Ch·∫ø ƒë·ªô `RoundRobin` ƒë·ªÉ lu√¢n phi√™n l∆∞·ª£t. | S·ª≠ d·ª•ng `PairwiseComparisonMetric` (DeepEval) ƒë·ªÉ so s√°nh A/B. |
| **Red Teaming** | Loop li√™n t·ª•c cho ƒë·∫øn khi t√¨m ƒë∆∞·ª£c l·ªó h·ªïng ho·∫∑c h·∫øt retry. | Load `Attacker Persona` (DAN/Hacker). TƒÉng `temperature` c·ªßa Sim ƒë·ªÉ s√°ng t·∫°o ƒë√≤n t·∫•n c√¥ng. | S·ª≠ d·ª•ng `ToxicityMetric`, `BiasMetric` l√†m ti√™u ch√≠ ch·∫∑n ƒë·ª©ng (Fail Criteria). |
| **Benchmarks** | Linear Pipeline (No branching). ƒê·ªçc t·ª´ file static JSONL. | T·∫Øt Simulation Worker (Bypass). Ch·ªâ d√πng Orchestrator g·ª≠i request th·∫≥ng. | S·ª≠ d·ª•ng `ExactMatch`, `F1 Score` (Strict grading). |
| **Human Review** | Th√™m node `WaitForHuman` v√†o graph n·∫øu ƒëi·ªÉm Confidence th·∫•p. Tr·∫°ng th√°i `SUSPEND`. | N/A | Log k·∫øt qu·∫£ `Pending` v√†o Database. Webhook b√°o Tester. |

---

## 3. Database Design (Schema Level)

### 3.1. PostgreSQL (Relational Data)

H·ªá th·ªëng s·ª≠ d·ª•ng **SQLModel** (SQLAlchemy + Pydantic) ƒë·ªÉ ƒë·ªãnh nghƒ©a Schema. C√°c b·∫£ng ch√≠nh bao g·ªìm:

#### `users` Table
Qu·∫£n l√Ω th√¥ng tin ng∆∞·ªùi d√πng (Google OAuth).

| Column | Type | Description |
| :--- | :--- | :--- |
| `id` | String (UUID) | Primary Key |
| `email` | String | Email (Unique) |
| `name` | String | Display Name |
| `avatar_url` | String | URL Avatar t·ª´ Google |
| `google_id` | String | Google OAuth ID |
| `created_at` | DateTime | Th·ªùi ƒëi·ªÉm ƒëƒÉng k√Ω |

#### `workspaces` Table
Qu·∫£n l√Ω kh√¥ng gian l√†m vi·ªác (Team/Project).

| Column | Type | Description |
| :--- | :--- | :--- |
| `id` | String (UUID) | Primary Key |
| `name` | String | T√™n Workspace |
| `owner_id` | String | FK t·ªõi b·∫£ng `users` (Owner) |
| `is_personal` | Boolean | True n·∫øu l√† Workspace m·∫∑c ƒë·ªãnh |
| `created_at` | DateTime | Th·ªùi ƒëi·ªÉm t·∫°o |

#### `workspace_members` Table
Qu·∫£n l√Ω th√†nh vi√™n trong Workspace.

| Column | Type | Description |
| :--- | :--- | :--- |
| `workspace_id` | String | FK t·ªõi b·∫£ng `workspaces` |
| `user_id` | String | FK t·ªõi b·∫£ng `users` |
| `role` | String | OWNER, EDITOR, VIEWER |
| `joined_at` | DateTime | Th·ªùi ƒëi·ªÉm tham gia |

#### `plans` Table
Qu·∫£n l√Ω c·∫•u h√¨nh c√°c g√≥i d·ªãch v·ª• (Free, Pro, Enterprise).

| Column | Type | Description |
| :--- | :--- | :--- |
| `id` | String (UUID) | Primary Key |
| `name` | String | T√™n g√≥i d·ªãch v·ª• (Free, Pro, Enterprise) |
| `price_monthly` | Float | Gi√° theo th√°ng (USD) |
| `price_annual` | Float | Gi√° theo nƒÉm (USD) |
| `features` | JSONB | C·∫•u h√¨nh gi·ªõi h·∫°n (max_workspaces, max_scenarios, max_runs_per_month, custom_metrics, ...) |

#### `subscriptions` Table
Qu·∫£n l√Ω tr·∫°ng th√°i g√≥i c∆∞·ªõc hi·ªán t·∫°i c·ªßa t·ªï ch·ª©c/Workspace.

| Column | Type | Description |
| :--- | :--- | :--- |
| `id` | String (UUID) | Primary Key |
| `workspace_id` | String | FK t·ªõi b·∫£ng `workspaces` (1 Workspace c√≥ 1 Sub active) |
| `plan_id` | String | FK t·ªõi b·∫£ng `plans` |
| `status` | String | active, past_due, canceled, pending |
| `paypal_sub_id`| String | ID subscription tr·∫£ v·ªÅ t·ª´ h·ªá th·ªëng PayPal |
| `period_start` | DateTime | Chu k·ª≥ t√≠nh c∆∞·ªõc hi·ªán t·∫°i (B·∫Øt ƒë·∫ßu) |
| `period_end`   | DateTime | Chu k·ª≥ t√≠nh c∆∞·ªõc hi·ªán t·∫°i (K·∫øt th√∫c) |

#### `agents` Table (AgentRef)
L∆∞u tr·ªØ th√¥ng tin chi ti·∫øt v·ªÅ c√°c target chatbot c·∫ßn ƒë∆∞·ª£c ƒë√°nh gi√°. **C√≥ th√™m `workspace_id`**.

| Column | Type | Description |
| :--- | :--- | :--- |
| `id` | String (UUID) | Primary Key |
| `workspace_id` | String | FK t·ªõi b·∫£ng `workspaces` (Ownership) |
| `name` | String | T√™n Agent |
| `endpoint_url` | String | Webhook/API URL c·ªßa Agent |
| `api_key_encrypted` | String | API Key ƒë√£ m√£ h√≥a (Fernet) |
| `type` | String | RAG Chatbot, v.v. |
| `status` | String | active, maintenance, deprecated |
| `langfuse_project_id`| String | ID d·ª± √°n t∆∞∆°ng ·ª©ng tr√™n Langfuse |
| `meta_data` | JSONB | C·∫•u h√¨nh b·ªï sung |

**(T∆∞∆°ng t·ª±, c√°c b·∫£ng `knowledge_bases`, `scenarios`, `metrics`, `campaigns` ƒë·ªÅu s·∫Ω c√≥ th√™m c·ªôt `workspace_id` ƒë·ªÉ ph√¢n quy·ªÅn).**

#### `knowledge_bases` Table (KnowledgeBaseRef)
Qu·∫£n l√Ω c√°c t√†i li·ªáu RAG.

| Column | Type | Description |
| :--- | :--- | :--- |
| `id` | String (UUID) | Primary Key |
| `workspace_id` | String | FK t·ªõi Workspace |
| `name` | String | T√™n Knowledge Base |
| `source_path` | String | ƒê∆∞·ªùng d·∫´n l∆∞u tr·ªØ (S3/Local) |
| `vector_db_type` | String | Chroma, Qdrant, Pinecone |
| `chunking_strategy` | String | Chi·∫øn l∆∞·ª£c c·∫Øt nh·ªè t√†i li·ªáu |

#### `scenarios` Table (ScenarioRef)
L∆∞u tr·ªØ c·∫•u tr√∫c k·ªãch b·∫£n test (Graph Nodes & Edges).

| Column | Type | Description |
| :--- | :--- | :--- |
| `id` | String (UUID) | Primary Key |
| `workspace_id` | String | FK t·ªõi Workspace |
| `name` | String | T√™n k·ªãch b·∫£n |
| `nodes` | JSONB | Danh s√°ch c√°c Node (ReactFlow format) |
| `edges` | JSONB | Danh s√°ch c√°c Edge (ReactFlow format) |
| `agent_id` | String | Foreign Key t·ªõi b·∫£ng `agents` |

#### `metrics` Table (MetricConfigRef)
Th∆∞ vi·ªán c√°c ti√™u ch√≠ ƒë√°nh gi√°.

| Column | Type | Description |
| :--- | :--- | :--- |
| `id` | String | ID ƒë·ªãnh danh (vd: 'faithfulness') |
| `workspace_id` | String | FK t·ªõi Workspace (ho·∫∑c null n·∫øu l√† Global Metric) |
| `name` | String | T√™n hi·ªÉn th·ªã |
| `threshold` | Float | Ng∆∞·ª°ng ƒë·∫°t (m·∫∑c ƒë·ªãnh 0.5) |
| `config` | JSONB | C·∫•u h√¨nh Prompt/Params cho DeepEval |

#### `manual_reviews` Table
H·ªó tr·ª£ quy tr√¨nh Duy·ªát th·ªß c√¥ng (Human-in-the-loop).

| Column | Type | Description |
| :--- | :--- | :--- |
| `id` | String (UUID) | Primary Key |
| `campaign_id` | String | Tham chi·∫øu t·ªõi Campaign |
| `auto_score` | Float | ƒêi·ªÉm s·ªë do AI ch·∫•m |
| `human_score` | Float | ƒêi·ªÉm s·ªë do con ng∆∞·ªùi ch·∫•m l·∫°i |
| `status` | String | pending, approved, rejected |

### 3.2. ClickHouse / NoSQL (Trace Data)

#### `traces` Table
L∆∞u tr·ªØ l·ªãch s·ª≠ chat chi ti·∫øt ƒë·ªÉ ph√¢n t√≠ch.

| Column | Type | Description |
| :--- | :--- | :--- |
| `trace_id` | UUID | Unique ID c·ªßa lu·ªìng |
| `span_id` | UUID | ID c·ªßa step (Thought/Action) |
| `event_type` | String | 'start', 'end', 'llm_call', 'tool_call' |
| `input` | String | User Input / Prompt |
| `output` | String | LLM Response |
| `latency_ms` | Float | Th·ªùi gian ph·∫£n h·ªìi |
| `cost_usd` | Float | Chi ph√≠ token |
| `metadata` | JSON | Tags, Annotations |

---

## 4. Class Design & Implementation Details

D∆∞·ªõi ƒë√¢y l√† thi·∫øt k·∫ø chi ti·∫øt (Class-level) cho c√°c service quan tr·ªçng nh·∫•t.

### 4.1. Orchestrator Service (Core Logic)

**Responsibility**: Qu·∫£n l√Ω v√≤ng ƒë·ªùi b√†i test b·∫±ng LangGraph.

```python
# orchestrator/app/services/workflow.py

class CampaignWorkflow:
    def __init__(self, scenario_config: dict):
        self.config = scenario_config
        self.builder = StateGraph(AgentState)
        self._setup_graph()

    def _setup_graph(self):
        """Kh·ªüi t·∫°o c√°c node v√† edge d·ª±a tr√™n k·ªãch b·∫£n"""
        # Th√™m c√°c Node x·ª≠ l√Ω Logic
        self.builder.add_node("simulation", simulation_node)
        self.builder.add_node("evaluation", evaluation_node)

        # ƒê·ªãnh nghƒ©a lu·ªìng di chuy·ªÉn
        self.builder.set_entry_point("simulation")
        self.builder.add_edge("simulation", "evaluation")
        
        # Lu·ªìng l·∫∑p (Cyclic) n·∫øu ƒëi·ªÉm th·∫•p
        self.builder.add_conditional_edges(
            "evaluation",
            check_retry,
            {
                "retry": "simulation",
                "finish": END
            }
        )
```

### 4.2. Evaluation Worker (DeepEval Integration)

**Responsibility**: Ch·∫•m ƒëi·ªÉm t·ª± ƒë·ªông v√† h·ªó tr·ª£ Human-in-the-loop.

```python
# evaluation-worker/app/services/evaluator.py

class EvaluationEngine:
    def evaluate(self, input_text, output_text, context, metrics):
        """S·ª≠ d·ª•ng DeepEval ƒë·ªÉ ch·∫•m ƒëi·ªÉm LLM-as-a-judge"""
        test_case = LLMTestCase(input=input_text, actual_output=output_text, retrieval_context=context)
        
        results = []
        for m in metrics:
            metric_impl = self._get_metric(m)
            metric_impl.measure(test_case)
            
            results.append({
                "score": metric_impl.score,
                "reason": metric_impl.reason,
                "low_confidence": metric_impl.score < CONFIDENCE_THRESHOLD
            })
        return results
```

### 4.3. Simulation Service (User Simulator)

**Responsibility**: Gi·∫£ l·∫≠p ng∆∞·ªùi d√πng (AutoGen).

```python
# simulation/agent_factory.py

class UserSimulatorFactory:
    @staticmethod
    def create_user(persona: dict):
        """Kh·ªüi t·∫°o AutoGen Agent v·ªõi t√≠nh c√°ch c·ª• th·ªÉ"""
        system_prompt = f"""
        B·∫°n l√† {persona['name']}. T√≠nh c√°ch: {persona['behavior']}.
        M·ª•c ti√™u c·ªßa b·∫°n l√†: {persona['goal']}.
        ƒê·ª´ng d·ªÖ d√†ng th·ªèa hi·ªáp. H√£y th·ª≠ th√°ch Bot.
        """
        
        return UserProxyAgent(
            name="SimUser",
            system_message=system_prompt,
            human_input_mode="NEVER",
            code_execution_config=False
        )
```

### 4.4. Resource Service (CRUD & Pagination)

**Responsibility**: Qu·∫£n l√Ω t√†i nguy√™n v√† k·∫øt n·ªëi Langfuse.

```python
# resource_service/app/api/v1/endpoints/agents.py

@router.get("/agents", response_model=Page[AgentRef])
async def list_agents(
    page: int = 1, 
    size: int = 10, 
    workspace_id: str = Header(...), # Require workspace context
    db: Session = Depends(get_db)
):
    """L·∫•y danh s√°ch Agent trong Workspace h·ªó tr·ª£ ph√¢n trang"""
    return agent_service.get_multi_by_workspace(db, workspace_id, page=page, size=size)

@router.post("/agents", response_model=AgentRef)
async def create_agent(
    config: AgentCreate, 
    workspace_id: str = Header(...),
    db: Session = Depends(get_db)
):
    """T·∫°o Agent v√† m√£ h√≥a API Key"""
    return agent_service.create_in_workspace(db, workspace_id, obj_in=config)
```

### 4.5. Data Ingestion Service (High-Throughput)

**Responsibility**: Nh·∫≠n Log t·ª´ Kafka v√† ghi v√†o Clickhouse (Batch Insert).
**Tech Stack**: Golang + Sarama (Kafka Client) + Go-ClickHouse.

```go
// data_ingestion/main.go

func main() {
    consumer := kafka.NewConsumer("traces-topic")
    batcher := NewBatcher(1000, 5*time.Second) // Flush m·ªói 1000 items ho·∫∑c 5s

    for msg := range consumer.Messages() {
        trace := ParseTrace(msg.Value)
        batcher.Add(trace)
        
        if batcher.Ready() {
            go ClickHouseRepository.BulkInsert(batcher.Flush())
        }
    }
}

type TraceLog struct {
    TraceID   string    `json:"trace_id"`
    WorkspaceID string  `json:"workspace_id"` // Add this
    Input     string    `json:"input"`
    Output    string    `json:"output"`
    Latency   float64   `json:"latency"`
    Timestamp time.Time `json:"timestamp"`
}
```

### 4.6. GenAI Service (Synthetic Data)

**Responsibility**: Sinh d·ªØ li·ªáu gi·∫£ l·∫≠p t·ª´ t√†i li·ªáu (RAG Testing).

```python
# genai_service/core/synthesizer.py

from langchain.document_loaders import PyPDFLoader
from deepeval.synthesizer import Synthesizer

class DatasetGenerator:
    def __init__(self, document_path: str):
        self.loader = PyPDFLoader(document_path)
        self.documents = self.loader.load_and_split()
        self.synthesizer = Synthesizer()

    async def generate_goldens(self, count: int = 50):
        """Sinh c·∫∑p c√¢u h·ªèi/c√¢u tr·∫£ l·ªùi t·ª´ t√†i li·ªáu"""
        goldens = await self.synthesizer.a_generate_goldens_from_docs(
            document_paths=[self.loader.file_path],
            max_goldens_per_document=count
        )
        return [
            {
                "input": g.input,
                "expected_output": g.expected_output,
                "context": g.context
            }
            for g in goldens
        ]
```

---

## 5. Feature Implementation List

Backend c·∫ßn implement c√°c API sau ƒë·ªÉ ph·ª•c v·ª• Frontend:

### A. Auth & Users (`/api/v1`)
- [x] `GET /health`: Health check.
- [x] `POST /auth/google`: Handle Google OAuth Login, create/update User.
- [x] `GET /me`: User Profile & Default Workspace.

### B. Workspace Management (`/api/v1/workspaces`)
- [x] `POST /workspaces`: Create new team workspace.
- [x] `GET /workspaces`: List my workspaces.
- [x] `POST /workspaces/{id}/invite`: Invite member by Email (Basic).
- [ ] `DELETE /workspaces/{id}/members/{user_id}`: Remove member.
- [ ] `PATCH /workspaces/{id}/members/{user_id}/role`: Update Role (Viewer -> Editor).

### C. Billing & Subscriptions (`/api/v1/billing`) *[NEW]*
- [ ] `GET /billing/plans`: L·∫•y danh s√°ch c√°c ƒë·ªãnh nghƒ©a g√≥i (Free, Pro, Enterprise) k√®m Options.
- [ ] `GET /billing/subscription`: L·∫•y t√¨nh tr·∫°ng Subscriptions hi·ªán t·∫°i c·ªßa Workspace v√† Tracker dung l∆∞·ª£ng (Usage Limit/Quota).
- [ ] `POST /billing/checkout`: T·∫°o link/order thanh to√°n PayPal.
- [ ] `POST /billing/webhook`: ƒê·∫ßu API ƒë·ªÉ nh·∫≠n Webhook t·ª´ PayPal th√¥ng b√°o tr·∫°ng th√°i thanh to√°n (Renewals, Cancellations).

### D. Agents & Resources (`/api/v1/resource/agents`)
- [x] `POST /resource/agents`: Create new agent config (Encrypted).
- [x] `GET /resource/agents`: List monitored agents (Pagination).
- [ ] **Must validate `Permission` (Editor/Owner) before Update/Delete.**

### D. Evaluation Campaigns (`/api/v1/orchestrator/campaigns`)
- [x] `POST /orchestrator/campaigns`: Queue a new testing job.
- [x] `GET /orchestrator/health`: Health check.
- [ ] `GET /orchestrator/campaigns/:id/state`: Streaming logs (Planned).

### D. Dataset Generator (`/api/v1/gen-ai/generate`)
- [x] `POST /generate/personas`: Sinh nh√¢n v·∫≠t gi·∫£ l·∫≠p.
- [x] `POST /generate/test-cases`: Sinh k·ªãch b·∫£n test.
- [x] `POST /optimize`: T·ªëi ∆∞u Prompt (GEPA).

### E. Metrics & Models (`/api/v1/resource`)
- [x] `GET/POST /resource/metrics-library`: Qu·∫£n l√Ω Metrics.
- [x] `GET/POST /resource/models`: Qu·∫£n l√Ω LLM Models.

### F. Human Review (`/api/v1/resource/reviews`)
- [x] `GET /resource/reviews/manual-reviews`: L·∫•y danh s√°ch queue.
- [x] `POST /resource/reviews/manual-reviews/{id}/decision`: Duy·ªát k·∫øt qu·∫£.

### G. Trace Debugger (`/api/v1/resource/traces`)
- [x] `GET /resource/traces`: List traces from Langfuse.
- [x] `GET /resource/traces/:id`: Get full trace detail.

---

## 6. Deployment Configuration

### 6.1. Docker Structure
C·∫•u tr√∫c th∆∞ m·ª•c ƒë·ªÅ xu·∫•t:

```
/backend
  /orchestrator (LangGraph)
    Dockerfile
  /workers
    /simulation (AutoGen)
      Dockerfile
    /evaluation (DeepEval)
      Dockerfile
  /shared
    /database (Models)
```

### 6.2. Dockerfile Example (Orchestrator)

```dockerfile
# backend/orchestrator/Dockerfile
FROM python:3.10-slim

WORKDIR /app

# Install System Dependencies
RUN apt-get update && apt-get install -y gcc libpq-dev

# Install Python Deps
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy Code
COPY . .

# Run
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8001"]
```


### 6.3. Docker Compose (Full Stack)

```yaml
services:
  # --- Data Layer ---
  postgres:
    container_name: postgres
    image: postgres:15
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-admin}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}
      POSTGRES_DB: ${POSTGRES_DB:-eval_db}
    volumes:
      - postgres:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
    ports:
      - "5432:5432"
    networks:
      - app-network
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-admin} -d ${POSTGRES_DB:-eval_db}" ]
      interval: 5s
      timeout: 5s
      retries: 5

  clickhouse:
    container_name: clickhouse
    image: clickhouse/clickhouse-server:23.8
    restart: always
    environment:
      CLICKHOUSE_DB: logs_db
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: "password"
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - clickhouse:/var/lib/clickhouse
    networks:
      - app-network

  redis:
    container_name: redis
    image: redis:7-alpine
    restart: always
    command: redis-server --requirepass myredissecret
    ports:
      - "6379:6379"
    volumes:
      - redis:/data
    networks:
      - app-network

  # --- Event Bus ---
  zookeeper:
    container_name: zookeeper
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - app-network

  kafka:
    container_name: kafka
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - app-network
    healthcheck:
      test: kafka-broker-api-versions --bootstrap-server localhost:9092 || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    ports:
      - "8082:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - app-network

  # --- Observability ---
  minio:
    container_name: minio
    image: minio/minio
    restart: always
    ports:
      - "9090:9000"
      - "9001:9001"
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: miniosecret
    volumes:
      - minio:/data
    networks:
      - app-network

  minio-init:
    container_name: minio-init
    image: minio/mc
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c " until (/usr/bin/mc alias set myminio http://minio:9000 minio miniosecret) do sleep 1; done; /usr/bin/mc mb --ignore-existing myminio/langfuse-events; /usr/bin/mc anonymous set public myminio/langfuse-events; "
    networks:
      - app-network

  langfuse-server:
    container_name: langfuse-server
    image: langfuse/langfuse:3
    depends_on:
      - postgres
      - redis
      - minio
      - clickhouse
    environment:
      - DATABASE_URL=postgresql://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-password}@postgres:5432/langfuse_db
      - NEXTAUTH_URL=http://localhost:3000
      - NEXTAUTH_SECRET=${NEXTAUTH_SECRET:-supersecret}
      - SALT=${SALT:-supersecret}
      - ENCRYPTION_KEY=bb306d480f3d47c3743a06cdf6891b038d3795caf51f17c39d8912d847d4848c
      - TELEMETRY_ENABLED=false
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_AUTH=myredissecret
      # ClickHouse Configuration
      - CLICKHOUSE_URL=http://clickhouse:8123
      - CLICKHOUSE_MIGRATION_URL=clickhouse://clickhouse:9000
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD=password
      - CLICKHOUSE_CLUSTER_ENABLED=false
      # MinIO / S3 Configuration for V3
      - LANGFUSE_S3_EVENT_UPLOAD_BUCKET=langfuse-events
      - LANGFUSE_S3_EVENT_UPLOAD_REGION=auto
      - LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID=minio
      - LANGFUSE_S3_EVENT_UPLOAD_SECRET_ACCESS_KEY=miniosecret
      - LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT=http://minio:9000
      - LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE=true
      - DISABLE_LATEST_RELEASE_CHECK=true
    ports:
      - "3000:3000"
    networks:
      - app-network

  langfuse-worker:
    container_name: langfuse-worker
    image: langfuse/langfuse-worker:3
    depends_on:
      - postgres
      - redis
      - minio
      - clickhouse
    environment:
      - DATABASE_URL=postgresql://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-password}@postgres:5432/langfuse_db
      - NEXTAUTH_SECRET=${NEXTAUTH_SECRET:-supersecret}
      - SALT=${SALT:-supersecret}
      - ENCRYPTION_KEY=bb306d480f3d47c3743a06cdf6891b038d3795caf51f17c39d8912d847d4848c
      - TELEMETRY_ENABLED=false
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_AUTH=myredissecret
      # ClickHouse Configuration
      - CLICKHOUSE_URL=http://clickhouse:8123
      - CLICKHOUSE_MIGRATION_URL=clickhouse://clickhouse:9000
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD=password
      - CLICKHOUSE_CLUSTER_ENABLED=false
      # MinIO / S3 Configuration for V3
      - LANGFUSE_S3_EVENT_UPLOAD_BUCKET=langfuse-events
      - LANGFUSE_S3_EVENT_UPLOAD_REGION=auto
      - LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID=minio
      - LANGFUSE_S3_EVENT_UPLOAD_SECRET_ACCESS_KEY=miniosecret
      - LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT=http://minio:9000
      - LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE=true
      - DISABLE_LATEST_RELEASE_CHECK=true
    networks:
      - app-network

  # --- Core Services ---
  orchestrator:
    container_name: orchestrator
    build: ./orchestrator
    ports:
      - "8001:8000"
    environment:
      - REDIS_URL=redis://:myredissecret@redis:6379
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS:-kafka:29092}
      - KAFKA_TOPIC_SIMULATION_REQUESTS=${KAFKA_TOPIC_SIMULATION_REQUESTS:-simulation.requests}
      - KAFKA_TOPIC_EVALUATION_REQUESTS=${KAFKA_TOPIC_EVALUATION_REQUESTS:-evaluation.requests}
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
      - LANGFUSE_HOST=${LANGFUSE_HOST}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      kafka:
        condition: service_healthy
      redis:
        condition: service_started
    networks:
      - app-network
    volumes:
      - ./orchestrator:/app

  identity-service:
    container_name: identity-service
    build: ./identity-service
    ports:
      - "8002:8000"
    environment:
      - DATABASE_URL=postgresql://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-password}@postgres:5432/${POSTGRES_DB:-eval_db}
    depends_on:
      - postgres
    networks:
      - app-network
    volumes:
      - ./identity-service:/app

  resource-service:
    container_name: resource-service
    build: ./resource-service
    ports:
      - "8003:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL}
      - MODEL_NAME=${MODEL_NAME:-deepseek-chat}
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
      - LANGFUSE_HOST=${LANGFUSE_HOST:-http://langfuse-server:3000}
      - DATABASE_URL=postgresql://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-password}@postgres:5432/${POSTGRES_DB:-eval_db}
    volumes:
      - ./resource-service:/app
    depends_on:
      - redis
      - kafka
      - langfuse-server
    networks:
      - app-network

  # --- Workers ---
  simulation-worker:
    container_name: simulation-worker
    build: ./simulation-worker
    ports:
      - "8004:8000"
    environment:
      - REDIS_URL=redis://:myredissecret@redis:6379
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS:-kafka:29092}
      - KAFKA_TOPIC_SIMULATION_REQUESTS=${KAFKA_TOPIC_SIMULATION_REQUESTS:-simulation.requests}
      - KAFKA_TOPIC_SIMULATION_COMPLETED=${KAFKA_TOPIC_SIMULATION_COMPLETED:-simulation.completed}
      - KAFKA_TOPIC_TRACES=${KAFKA_TOPIC_TRACES:-traces}
      - KAFKA_GROUP_SIMULATION=${KAFKA_GROUP_SIMULATION:-simulation-group}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL}
      - MODEL_NAME=${MODEL_NAME:-deepseek-chat}
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
      - LANGFUSE_HOST=${LANGFUSE_HOST}
    depends_on:
      - orchestrator
    networks:
      - app-network
    volumes:
      - ./simulation-worker:/app

  evaluation-worker:
    container_name: evaluation-worker
    build: ./evaluation-worker
    deploy:
      replicas: 1
    ports:
      - "8005:8000"
    environment:
      - REDIS_URL=redis://:myredissecret@redis:6379
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS:-kafka:29092}
      - KAFKA_TOPIC_EVALUATION_REQUESTS=${KAFKA_TOPIC_EVALUATION_REQUESTS:-evaluation.requests}
      - KAFKA_TOPIC_EVALUATION_COMPLETED=${KAFKA_TOPIC_EVALUATION_COMPLETED:-evaluation.completed}
      - KAFKA_TOPIC_TRACES=${KAFKA_TOPIC_TRACES:-traces}
      - KAFKA_GROUP_EVALUATION=${KAFKA_GROUP_EVALUATION:-evaluation-group}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL}
      - MODEL_NAME=${MODEL_NAME:-deepseek-chat}
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
      - LANGFUSE_HOST=${LANGFUSE_HOST}
    depends_on:
      kafka:
        condition: service_healthy
      redis:
        condition: service_started
      langfuse-server:
        condition: service_started
    networks:
      - app-network
    volumes:
      - ./evaluation-worker:/app

  gen-ai-service:
    container_name: gen-ai-service
    build: ./gen-ai-service
    ports:
      - "8006:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL}
      - MODEL_NAME=${MODEL_NAME:-deepseek-chat}
    networks:
      - app-network
    volumes:
      - ./gen-ai-service:/app

  data-ingestion:
    container_name: data-ingestion
    build: ./data-ingestion
    ports:
      - "8008:8080"
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS:-kafka:29092}
      - KAFKA_TOPIC_TRACES=${KAFKA_TOPIC_TRACES:-traces}
      - KAFKA_GROUP_INGESTION=${KAFKA_GROUP_INGESTION:-ingestion-group-rust}
      - CLICKHOUSE_URL=${CLICKHOUSE_URL:-http://clickhouse:8123}
    depends_on:
      kafka:
        condition: service_healthy
      clickhouse:
        condition: service_started
    networks:
      - app-network
    volumes:
      - data-ingestion:/app

volumes:
  postgres:
    name: postgres
  clickhouse:
    name: clickhouse
  redis:
    name: redis
  minio:
    name: minio
  orchestrator:
    name: orchestrator
  identity-service:
    name: identity-service
  resource-service:
    name: resource-service
  simulation-worker:
    name: simulation-worker
  evaluation-worker:
    name: evaluation-worker
  gen-ai-service:
    name: gen-ai-service
  data-ingestion:
    name: data-ingestion

networks:
  app-network:
    driver: bridge
```

### 6.4. Service Port Mapping

| Service Name | Host Port | Container Port | Description |
| :--- | :--- | :--- | :--- |
| **PostgreSQL** | `5432` | `5432` | Relational Database |
| **ClickHouse** | `8123`, `9000` | `8123`, `9000` | Analytics Database |
| **Redis** | `6379` | `6379` | Cache & Message Broker / State Store |
| **Kafka** | `9092` | `9092` | Event Streaming Platform |
| **Kafka UI** | `8082` | `8080` | Kafka Management UI |
| **MinIO** | `9090` (API), `9001` (Console) | `9000`, `9001` | S3-compatible Object Storage |
| **Langfuse Server** | `3000` | `3000` | Observability Platform (UI/API) |
| **Orchestrator** | `8001` | `8000` | Core Logic / LangGraph Engine |
| **Identity Service** | `8002` | `8000` | User Management & Auth |
| **Resource Service** | `8003` | `8000` | CRUD for Resources (Agents, Models) |
| **Simulation Worker** | `8004` | `8000` | User Simulator (auto-gen) |
| **Evaluation Worker** | `8005` | `8000` | LLM-as-a-Judge Worker |
| **GenAI Service** | `8006` | `8000` | Synthetic Data Generator |
| **Data Ingestion** | `8008` | `8080` | High-throughput Log Ingestion |
| **Billing Service** | `8009` | `8000` | PayPal Integration & Subscriptions Management |

---

## 7. Testing & Data Seeding

H·ªá th·ªëng cung c·∫•p b·ªô c√¥ng c·ª• ki·ªÉm th·ª≠ v√† kh·ªüi t·∫°o d·ªØ li·ªáu m·∫´u t·∫°i th∆∞ m·ª•c `backend/tests/`.

### 7.1. Data Seeding Scripts
D√πng ƒë·ªÉ chu·∫©n b·ªã m√¥i tr∆∞·ªùng demo ho·∫∑c testing:
- **`seed_data.py`**: Kh·ªüi t·∫°o th√¥ng tin c∆° b·∫£n cho Agents v√† Scenarios.
- **`seed_metrics.py`**: N·∫°p c√°c ƒë·ªãnh nghƒ©a metrics chu·∫©n (Faithfulness, Relevancy) v√†o th∆∞ vi·ªán.
- **`seed_manual_review.py`**: Gi·∫£ l·∫≠p c√°c b·∫£n ghi c·∫ßn duy·ªát th·ªß c√¥ng ƒë·ªÉ test UI.

### 7.2. Verification & E2E Tests
C√°c k·ªãch b·∫£n ki·ªÉm tra t√≠nh to√†n v·∫πn c·ªßa h·ªá th·ªëng:
- **`test_e2e.py`**: Ch·∫°y lu·ªìng ƒë√°nh gi√° t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi (User Sim -> Evaluator).
- **`verify_system_integration.py`**: Ki·ªÉm tra k·∫øt n·ªëi gi·ªØa c√°c microservices v√† h·∫° t·∫ßng (Kafka, Redis, Postgres).
- **`verify_agents_crud.py`**: Ki·ªÉm tra logic CRUD v√† m√£ kh√≥a API Key c·ªßa Resource Service.

### 7.3. Security Scanning
- **`security_scan.sh`**: Script t·ª± ƒë·ªông ch·∫°y qu√©t c√°c l·ªó h·ªïng OWASP/Dependabot cho c√°c service.

---

## 8. T·ªïng k·∫øt Hi·ªán tr·∫°ng Tri·ªÉn khai (Implementation Status)

D·ª±a tr√™n th·ª±c t·∫ø m√£ ngu·ªìn, h·ªá th·ªëng ƒë√£ ho√†n thi·ªán c√°c th√†nh ph·∫ßn c·ªët l√µi (Core Foundations) v√† ƒëang chuy·ªÉn sang giai ƒëo·∫°n t·ªëi ∆∞u h√≥a tr·∫£i nghi·ªám ng∆∞·ªùi d√πng.

### 8.1. T·ªïng quan theo Service

| Service | Tr·∫°ng th√°i | Version | C√°c th√†nh ph·∫ßn n·ªïi b·∫≠t |
| :--- | :--- | :--- | :--- |
| **Resource Service** | üü¢ Production Ready | v1.2 | CRUD ƒë·∫ßy ƒë·ªß, Pagination, Langfuse Integration, API Key Encryption |
| **Orchestrator** | üü¢ Production Ready | v1.0 | LangGraph, Dynamic Graph Builder, Redis Persistence, Cyclic Logic |
| **Evaluation Worker** | üü¢ Production Ready | v1.0 | DeepEval Integration, Human-in-the-loop, Batch Processing |
| **Simulation Worker** | üü¢ Production Ready | v1.0 | AutoGen, Red Teaming, Multi-agent Chat, Persona System |
| **Gen AI Service** | üü¢ Production Ready | v1.0 | Persona/TestCase Generation, RAG Integration, Prompt Optimization |
| **Identity Service** | üü° In Progress | v0.8 | Entra ID Auth, Auto-provisioning (Thi·∫øu: Team Management, RBAC) |
| **Data Ingestion** | üü° In Progress | v0.7 | Kafka Consumer, ClickHouse Batch Insert (Thi·∫øu: Parsing Logic, DLQ) |

### 8.2. Chi ti·∫øt Tr·∫°ng th√°i t·ª´ng Service

#### üü¢ Resource Service (Production Ready - v1.2)
**ƒê√£ ho√†n thi·ªán:**
- ‚úÖ Full CRUD APIs cho Agents, Knowledge Bases, Scenarios, Models, Metrics
- ‚úÖ Generic Pagination System v·ªõi `Page[T]` response model
- ‚úÖ API Key Encryption b·∫±ng Fernet (cryptography library)
- ‚úÖ Langfuse Integration ƒë·ªÉ fetch trace data
- ‚úÖ Advanced Validation (Pydantic strict typing, regex validators)
- ‚úÖ Vector DB support (ChromaDB, Qdrant, Pinecone)
- ‚úÖ Manual Review workflow endpoints

**ƒêang ph√°t tri·ªÉn:**
- üîÑ JWT Authentication Middleware
- üîÑ Role-based Access Control cho endpoints
- üîÑ Chu·∫©n h√≥a Error Messages 100% ti·∫øng Vi·ªát

**Code Coverage:** Models (100%), CRUD APIs (100%), Langfuse (100%), Validation (70%)

---

#### üü¢ Orchestrator (Production Ready - v1.0)
**ƒê√£ ho√†n thi·ªán:**
- ‚úÖ LangGraph workflow engine v·ªõi State Management
- ‚úÖ Dynamic Graph Builder t·ª´ JSON configuration
- ‚úÖ Redis Persistence (RedisSaver) cho checkpoint v√† time-travel debugging
- ‚úÖ Async Kafka Integration (Producer/Consumer)
- ‚úÖ Cyclic Logic (Self-correction) v·ªõi retry mechanism
- ‚úÖ Campaign Management API (`POST /orchestrator/campaigns`)
- ‚úÖ Langfuse tracing cho workflow execution
- ‚úÖ **Red Teaming Workflow (FR-04)**:
  - `build_red_teaming_graph` implementation v·ªõi specialized state machine
  - Adversarial attack orchestration (Jailbreak, Prompt Injection, PII Leakage, Toxicity)
  - Integration v·ªõi Evaluation Worker ƒë·ªÉ ph√¢n lo·∫°i severity levels
  - Real-time progress tracking v√† database updates
  - Automated probe generation v√† attack execution loop
- ‚úÖ **Multi-Language Support (Localization)**:
  - `language` parameter support trong Campaign workflows
  - Dynamic instruction generation d·ª±a tr√™n ng√¥n ng·ªØ (English/Vietnamese)
  - Language propagation qua Kafka messages ƒë·∫øn Workers
  - Metadata tracking cho language preferences

**Roadmap:**
- üìã WebSocket/SSE streaming logs
- üìã Prometheus metrics export
- üìã Multi-campaign parallel execution

---

#### üü¢ Evaluation Worker (Production Ready - v1.0)
**ƒê√£ ho√†n thi·ªán:**
- ‚úÖ DeepEval framework integration (Faithfulness, Answer Relevancy, Hallucination)
- ‚úÖ LLM-as-a-Judge v·ªõi OpenAI/DeepSeek
- ‚úÖ Async Kafka Consumer v·ªõi Batch Processing (batch size: 10)
- ‚úÖ Redis caching cho evaluation results
- ‚úÖ Human-in-the-loop: Auto-route low confidence items (threshold: 0.5)
- ‚úÖ Retry mechanism v·ªõi exponential backoff
- ‚úÖ Langfuse callback tracking
- ‚úÖ **Red Teaming Evaluation (FR-04)**: Safety Metrics (Toxicity, PII, Bias), Severity classification, Refusal detection
- ‚úÖ **Multi-Language Support**: Language-aware evaluation prompts, support for multi-language content analysis

**Roadmap:**
- üìã Custom Metrics cho domain-specific evaluation
- üìã Parallel evaluation cho multiple test cases
- üìã LLM response caching ƒë·ªÉ gi·∫£m cost

---

#### üü¢ Simulation Worker (Production Ready - v1.0)
**ƒê√£ ho√†n thi·ªán:**
- ‚úÖ Microsoft AutoGen integration cho conversational agents
- ‚úÖ Persona System v·ªõi UserSimulatorFactory
- ‚úÖ **Red Teaming (FR-04)**: Adversarial personas, attack scenarios
- ‚úÖ Multi-agent Chat (GroupChat, GroupChatManager)
- ‚úÖ Dynamic connection t·ªõi target bots (HTTP/WebSocket)
- ‚úÖ Human Proxy support v·ªõi `human_input_mode`
- ‚úÖ Conversation management v·ªõi max turns limit
- ‚úÖ **Multi-Language Support**: Dynamic persona generation and scenario execution based on language parameter

**Roadmap:**
- üìã Dynamic Tool Loading t·ª´ external config
- üìã Garak/PyRIT integration cho advanced red teaming
- üìã Agent pooling ƒë·ªÉ gi·∫£m initialization overhead

---

#### üü¢ Gen AI Service (Production Ready - v1.0)
**ƒê√£ ho√†n thi·ªán:**
- ‚úÖ Persona Generation t·ª´ business context
- ‚úÖ Test Case Generation v·ªõi evolutionary approach
- ‚úÖ RAG Integration: PDF parsing (PyMuPDF), Vector Store (ChromaDB)
- ‚úÖ MinIO/S3 storage integration cho documents
- ‚úÖ Prompt Optimization v·ªõi GEPA algorithm
- ‚úÖ LangChain orchestration

**Roadmap:**
- üìã DeepEval self-check cho synthetic data quality
- üìã Hybrid Search (Keyword + Vector)
- üìã MIPROv2 algorithm upgrade

---

#### üü° Identity Service (In Progress - v0.8)
**ƒê√£ ho√†n thi·ªán:**
- ‚úÖ Microsoft Entra ID integration (JWT verification)
- ‚úÖ Auto-provisioning user khi ƒëƒÉng nh·∫≠p l·∫ßn ƒë·∫ßu
- ‚úÖ User Profile API (`GET /me`)
- ‚úÖ Database models (User, Team, Role)

**Ch∆∞a ho√†n thi·ªán (Gap Analysis):**
- ‚ùå Team Management APIs (`POST /team/invite`, `PATCH /team/role`)
- ‚ùå Fine-grained RBAC middleware
- ‚ùå API Key Management (FR-06)
- ‚ùå Audit logging

**∆Øu ti√™n:** High - C·∫ßn ho√†n thi·ªán ƒë·ªÉ support multi-user collaboration

---

#### üü° Data Ingestion (In Progress - v0.7)
**ƒê√£ ho√†n thi·ªán:**
- ‚úÖ Actix-web server v·ªõi Rust async runtime
- ‚úÖ Kafka Consumer (rdkafka) v·ªõi auto-commit
- ‚úÖ ClickHouse HTTP Client
- ‚úÖ Batch Insert logic (flush theo size: 1000 items ho·∫∑c time: 5s)
- ‚úÖ Prometheus metrics endpoint (`/metrics`)

**ƒêang ph√°t tri·ªÉn:**
- üîÑ JSON/Protobuf parsing logic ƒë·ªÉ map v√†o ClickHouse schema

**Ch∆∞a ho√†n thi·ªán:**
- ‚ùå Dead Letter Queue (DLQ) cho failed inserts
- ‚ùå Configuration externalization (Env Vars)
- ‚ùå Graceful shutdown (flush pending batch)
- ‚ùå Advanced monitoring metrics

**∆Øu ti√™n:** Medium - Service ho·∫°t ƒë·ªông nh∆∞ng thi·∫øu production-grade features

---

### 8.3. T·ªïng k·∫øt theo Nh√≥m Ch·ª©c nƒÉng

| Nh√≥m Ch·ª©c nƒÉng | Tr·∫°ng th√°i | Ghi ch√∫ |
| :--- | :--- | :--- |
| **Core Workflow** | üü¢ Complete | Orchestrator + Workers ho·∫°t ƒë·ªông end-to-end |
| **Resource Management** | üü¢ Complete | CRUD ƒë·∫ßy ƒë·ªß, pagination, validation |
| **AI Capabilities** | üü¢ Complete | Evaluation, Simulation, Generation ƒë·ªÅu production-ready |
| **Red Teaming** | üü¢ Complete | Adversarial testing v·ªõi 4 attack strategies, multi-language support |
| **Localization** | üü¢ Complete | English/Vietnamese support cho Scenarios v√† Red Teaming |
| **Observability** | üü¢ Complete | Langfuse integration, metrics endpoints |
| **Authentication** | üü° Partial | Basic auth c√≥, thi·∫øu RBAC v√† team management |
| **Data Pipeline** | üü° Partial | Ingestion ho·∫°t ƒë·ªông, thi·∫øu error handling n√¢ng cao |
| **Testing** | üü° Partial | Unit tests c√≥, thi·∫øu integration tests to√†n di·ªán |

### 8.4. Roadmap ∆Øu ti√™n

#### ∆Øu ti√™n Cao (Q1 2026)
1. **Identity Service**: Ho√†n thi·ªán Team Management v√† RBAC
2. **Data Ingestion**: Implement DLQ v√† graceful shutdown
3. **Integration Tests**: E2E testing cho to√†n b·ªô workflow

#### ∆Øu ti√™n Trung b√¨nh (Q2 2026)
1. **Streaming Logs**: WebSocket/SSE cho real-time updates
2. **Advanced Metrics**: Custom evaluation metrics
3. **Performance**: Caching, parallel execution

#### ∆Øu ti√™n Th·∫•p (Q3+ 2026)
1. **Advanced Red Teaming**: Garak/PyRIT integration
2. **Multi-modal RAG**: Image/Chart evaluation
3. **Self-optimization**: Automated prompt tuning

---

## 9. L·ªô tr√¨nh ph√°t tri·ªÉn t∆∞∆°ng lai (Product Roadmap)

L·ªô tr√¨nh ƒë∆∞·ª£c chia l√†m 3 giai ƒëo·∫°n chi·∫øn l∆∞·ª£c ƒë·ªÉ ƒë∆∞a s·∫£n ph·∫©m l√™n t·∫ßm Enterprise-Grade, t√≠ch h·ª£p to√†n di·ªán c√°c nƒÉng l·ª±c v·ªÅ Red Teaming, RAG v√† Dataset.

### Giai ƒëo·∫°n 2: Studio Experience (Hi·ªán t·∫°i - Q2/2026)
*T·∫≠p trung v√†o tr·∫£i nghi·ªám ng∆∞·ªùi d√πng No-code v√† quy tr√¨nh duy·ªát.*
- **AI Studio Web**: Ho√†n thi·ªán Visual Scenario Builder cho QA/Tester.
- **Human-in-the-loop**: Giao di·ªán Annotator UI cho ph√©p ghi ƒë√® (Override) ƒëi·ªÉm s·ªë c·ªßa AI Judge.
- **Team Management**: H·ªá th·ªëng m·ªùi th√†nh vi√™n v√† qu·∫£n l√Ω Role chi ti·∫øt.
- **Streaming Logs**: H·ªó tr·ª£ SSE/WebSocket ƒë·ªÉ xem lu·ªìng Thought -> Action c·ªßa Agent realtime.
- **Dataset Expansion**: Sinh d·ªØ li·ªáu ti·∫øn h√≥a (Evolutionary) t·ª± ƒë·ªông t·ª´ t√†i li·ªáu nghi·ªáp v·ª•.

### Giai ƒëo·∫°n 3: Scale & Ecosystem (Q3/2026+)
*M·ªü r·ªông quy m√¥, t√≠ch h·ª£p s√¢u v√† t·ªëi ∆∞u h√≥a.*
- **Battle Mode (Arena)**: Giao di·ªán so s√°nh m√π (Blind Comparison) gi·ªØa c√°c model ƒë·ªÉ t√≠nh ƒëi·ªÉm ELO.
- **Advanced Red Teaming**: T√≠ch h·ª£p c√°c th∆∞ vi·ªán t·∫•n c√¥ng chuy√™n s√¢u nh∆∞ Garak ho·∫∑c Microsoft PyRIT.
- **CI/CD Integration**: Cung c·∫•p Github Actions / Gitlab UI integration ƒë·ªÉ ch·∫∑n Merge Request n·∫øu ƒëi·ªÉm Eval th·∫•p.
- **Dynamic Tool Loading**: Kh·∫£ nƒÉng n·∫°p c√°c Function Calling Tools ƒë·ªông t·ª´ API b√™n ngo√†i.
- **RAG Optimization**: ƒê√°nh gi√° Hybrid Search, Reranking v√† t·ª± ƒë·ªông g√°n nh√£n (Auto-labeling) dataset.

### Giai ƒëo·∫°n 4: Self-Optimization & Intelligence (Q4/2026+)
*H·ªá th·ªëng t·ª± h·ªçc v√† t·ª± c·∫£i thi·ªán.*
- **Advanced Prompt Tuning**: N√¢ng c·∫•p thu·∫≠t to√°n MIPROv2 ƒë·ªÉ t·ª± ƒë·ªông ch·ªânh s·ª≠a Prompt cho Agent.
- **Retrieval Drift Monitoring**: T√≠ch h·ª£p **Arize Phoenix** ƒë·ªÉ gi√°m s√°t s·ª± thay ƒë·ªïi ch·∫•t l∆∞·ª£ng c·ªßa Vector Database.
- **Multi-modal RAG Eval**: ƒê√°nh gi√° kh·∫£ nƒÉng tr√≠ch xu·∫•t th√¥ng tin t·ª´ bi·ªÉu ƒë·ªì v√† h√¨nh ·∫£nh trong t√†i li·ªáu.
- **Synthetic Quality Assurance**: AI-Judge t·ª± ƒë·ªông ki·ªÉm ƒë·ªãnh ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu sinh ra (Self-check).

---
**Last Updated**: 2026-02-10
**By**: TuanTD
